{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Datapreparer Data preparation is the process of combining and cleaning data for analysis. This process is often time-consuming, so Datapreparer provides automation for many manual tasks. These docs illustrate the features and fundamental concepts of Datapreparer.","title":"Contributing"},{"location":"#welcome-to-datapreparer","text":"Data preparation is the process of combining and cleaning data for analysis. This process is often time-consuming, so Datapreparer provides automation for many manual tasks. These docs illustrate the features and fundamental concepts of Datapreparer.","title":"Welcome to Datapreparer"},{"location":"spring-setup/","text":"Application structure Datapreparer is a spring application written in Java. This page gives a basic introduction to the project and application structure. And a guide to the key components and packages of the application. Overview of the application After cloning the project and setting up the project in your IDE of choice (see here if you haven't done this), you should see an app directory. This is the root directory for the project. You should then see the src directory. If you open this folder, you should open this and see two subdirectories: main - which contains the source code for the application tests - which contains the test code for the spring application. If you navigate into main, you will see two folders: java and resources . Java contains source code for the application and resources contain all the static files and scripts required. Package overview Now that you have a general idea of the project structure, the next question is what do all the packages inside java do? Here is a general overview of each package and its function. Package Function app Contains the main configuration for spring and application entry point. client persistence Contains entities and repositories to handle data persistence and database connections. transducers Contains all of the transducers (see what is a transducer?). WebUI Web interface for datapreparer. component Contains a series of core components that manage application settings, connection pools, and other shared components. service Contains all of the services the application uses. In this context, a service is a spring service (essentially a container for business logic) Spring component diagram the spring application diagram will go here, will show the data tiers and from there we will give a more complete description Data persistence connect with Hibernate which is an implementation of JPA connection is by JDBC connections string to a H2SQL database this can cause means the database is in memory so all data will be lost once the application is shut down This means there is no way to save analysis of data preparation that are in progress. Also explain where the properties are to handle the data sources","title":"Spring Setup"},{"location":"spring-setup/#application-structure","text":"Datapreparer is a spring application written in Java. This page gives a basic introduction to the project and application structure. And a guide to the key components and packages of the application.","title":"Application structure"},{"location":"spring-setup/#overview-of-the-application","text":"After cloning the project and setting up the project in your IDE of choice (see here if you haven't done this), you should see an app directory. This is the root directory for the project. You should then see the src directory. If you open this folder, you should open this and see two subdirectories: main - which contains the source code for the application tests - which contains the test code for the spring application. If you navigate into main, you will see two folders: java and resources . Java contains source code for the application and resources contain all the static files and scripts required.","title":"Overview of the application"},{"location":"spring-setup/#package-overview","text":"Now that you have a general idea of the project structure, the next question is what do all the packages inside java do? Here is a general overview of each package and its function. Package Function app Contains the main configuration for spring and application entry point. client persistence Contains entities and repositories to handle data persistence and database connections. transducers Contains all of the transducers (see what is a transducer?). WebUI Web interface for datapreparer. component Contains a series of core components that manage application settings, connection pools, and other shared components. service Contains all of the services the application uses. In this context, a service is a spring service (essentially a container for business logic)","title":"Package overview"},{"location":"spring-setup/#spring-component-diagram","text":"the spring application diagram will go here, will show the data tiers and from there we will give a more complete description","title":"Spring component diagram"},{"location":"spring-setup/#data-persistence","text":"connect with Hibernate which is an implementation of JPA connection is by JDBC connections string to a H2SQL database this can cause means the database is in memory so all data will be lost once the application is shut down This means there is no way to save analysis of data preparation that are in progress. Also explain where the properties are to handle the data sources","title":"Data persistence"},{"location":"advanced/web-security/","text":"Web security By default, authentication is not enabled for datapreparer. However, you may want to ensure that only authenticated users can access the web interface. Datapreparer gives you the ability to define a list of usernames and passwords for intended users of the application. To do this, we edit the datapreparer-users.properties . This file is expected to have the form user.username = password . An example is: user.user1 = pass1 user.user2 = pass2 Enabling HTTPS To run this step, you need to have SSL certificates pre-installed on your server. And you have generated the application JAR. We can also enable HTTPS to run with datapreparer. It takes the following steps: Get SSL certicicates Get your SSL certificates from your chosen certificate provider. Adding certificate to the Keystore You now need to import the certificate into a keystore. Java often comes with a tool called keytool to do this. The command to do this is: keytool -import -alias myAlias -file myCertificate.crt -keystore /path/to/keystore.p12 -storepass password Modifying server properties First, Do the following two steps: vi datapreparer-${version}.jar . Choose the file BOOT-INF/classes/application.properties Modify the file based on the following # The format used for the Keystore. It could be set to JKS if it is a JKS file server.SSL.key-store-type=PKCS12 # The path to the Keystore containing the certificate server.SSL.key-store=/path/to/Keystore.p12 # The password used to generate the certificate server.SSL.key-store-password=password # The alias mapped to the certificate server.SSL.key-alias=myAlias Restarting the service (if running) You now restart datapreparer using sudo systemctl restart datapreparer.service Restarting the service (if running) You now just restart data preparer using sudo systemctl restart datapreparer.service","title":"Web security"},{"location":"advanced/web-security/#web-security","text":"By default, authentication is not enabled for datapreparer. However, you may want to ensure that only authenticated users can access the web interface. Datapreparer gives you the ability to define a list of usernames and passwords for intended users of the application. To do this, we edit the datapreparer-users.properties . This file is expected to have the form user.username = password . An example is: user.user1 = pass1 user.user2 = pass2","title":"Web security"},{"location":"advanced/web-security/#enabling-https","text":"To run this step, you need to have SSL certificates pre-installed on your server. And you have generated the application JAR. We can also enable HTTPS to run with datapreparer. It takes the following steps:","title":"Enabling HTTPS"},{"location":"advanced/web-security/#get-ssl-certicicates","text":"Get your SSL certificates from your chosen certificate provider.","title":"Get SSL certicicates"},{"location":"advanced/web-security/#adding-certificate-to-the-keystore","text":"You now need to import the certificate into a keystore. Java often comes with a tool called keytool to do this. The command to do this is: keytool -import -alias myAlias -file myCertificate.crt -keystore /path/to/keystore.p12 -storepass password","title":"Adding certificate to the Keystore"},{"location":"advanced/web-security/#modifying-server-properties","text":"First, Do the following two steps: vi datapreparer-${version}.jar . Choose the file BOOT-INF/classes/application.properties Modify the file based on the following # The format used for the Keystore. It could be set to JKS if it is a JKS file server.SSL.key-store-type=PKCS12 # The path to the Keystore containing the certificate server.SSL.key-store=/path/to/Keystore.p12 # The password used to generate the certificate server.SSL.key-store-password=password # The alias mapped to the certificate server.SSL.key-alias=myAlias","title":"Modifying server properties"},{"location":"advanced/web-security/#restarting-the-service-if-running","text":"You now restart datapreparer using sudo systemctl restart datapreparer.service","title":"Restarting the service (if running)"},{"location":"advanced/web-security/#restarting-the-service-if-running_1","text":"You now just restart data preparer using sudo systemctl restart datapreparer.service","title":"Restarting the service (if running)"},{"location":"advanced/command-line/commands/","text":"Commands Launch Datapreparer Linux or Mac: ./datapreparer.sh Windows: datapreparer.bat Command parameters: Command parameters Explaination -b or \u2013no-browser. The default behaviour in Data Preparer is that the home page of the web application, accessible at http://localhost:8182, is launched in the default browser. The -b parameter, however, prevents the browser from starting up, so as to complete a wrangling process purely in the command line. This could be used in conjunction with the -x option that exports the result to a csv file, or when the end product is written to a database. In the presence of the -b parameter, Data Preparer will exit having run a wrangling process on its inputs. -d or \u2013data-context filename filename is a path to a file that describes where the resources in the data context are. -e or \u2013export-scenario filename This will export filename, that is a Data Preparer scenario file, a compressed file containing the configuration options, the sources file, the data context file, the feedback provided by the user, and any csv files that have been uploaded from the web interface. This scenario file can subsequently be imported using the -i parameter, so that you can resume working with the scenario, or share it. -f or \u2013feedback filename filename is a path to a file that contains feedback instances. -h or \u2013help Prints a help message with the available command line options and exits. -i or \u2013import-scenario filename This initialises a wrangling scenario by loading the filename, which is a compressed file containing one or more of the following: a properties file with the wrangling configuration, a sources.csv file with information about how to access the sources, a datacontext.csv file with the information about how to access resources in the data context, a feedback.csv file with feedback instances, and finally any csv files that may have been uploaded from the web interface. This is complementary to the -e option that exports the scenario configuration. -l or \u2013license-key provides the license key to Data Preparer; a license key is emailed to the email address provided when downloading the software. The argument in this parameter provides either the license key itself or the path to a file containing the license key. In order not to have to specify the license key every time Data Preparer runs, a file containing \u2019license\u2019 in its name, can be placed in the same folder with the executable, containing the license key. -o or \u2013override property=value This loads the scenario and overrides one or more properties in the configuration. For instance, -o product.name=p1,product.size=200 will set the product name to p1 and the number of tuples to include in the end product to 200. Note that no spaces are allowed in the property-value pairs. -p or \u2013properties filename filename is a path to a properties file that configures the wrangling process. -q or \u2013quit Terminates any running Data Preparer instances. -r or \u2013server-port port-number This changes the port on which Data Preparer runs, from the default 8182 to port-number. For instance, running with -r 8183 means that the web interface will be accessible at http://localhost:8183. -s or \u2013sources filename filename is a path to a sources file that describes where the data sources are. -w or \u2013wrangle run the wrangling process. -x or \u2013export filename filename is the path to a csv file that should be used to store the result of the wrangling process.","title":"Commands"},{"location":"advanced/command-line/commands/#commands","text":"","title":"Commands"},{"location":"advanced/command-line/commands/#launch-datapreparer","text":"Linux or Mac: ./datapreparer.sh Windows: datapreparer.bat","title":"Launch Datapreparer"},{"location":"advanced/command-line/commands/#command-parameters","text":"Command parameters Explaination -b or \u2013no-browser. The default behaviour in Data Preparer is that the home page of the web application, accessible at http://localhost:8182, is launched in the default browser. The -b parameter, however, prevents the browser from starting up, so as to complete a wrangling process purely in the command line. This could be used in conjunction with the -x option that exports the result to a csv file, or when the end product is written to a database. In the presence of the -b parameter, Data Preparer will exit having run a wrangling process on its inputs. -d or \u2013data-context filename filename is a path to a file that describes where the resources in the data context are. -e or \u2013export-scenario filename This will export filename, that is a Data Preparer scenario file, a compressed file containing the configuration options, the sources file, the data context file, the feedback provided by the user, and any csv files that have been uploaded from the web interface. This scenario file can subsequently be imported using the -i parameter, so that you can resume working with the scenario, or share it. -f or \u2013feedback filename filename is a path to a file that contains feedback instances. -h or \u2013help Prints a help message with the available command line options and exits. -i or \u2013import-scenario filename This initialises a wrangling scenario by loading the filename, which is a compressed file containing one or more of the following: a properties file with the wrangling configuration, a sources.csv file with information about how to access the sources, a datacontext.csv file with the information about how to access resources in the data context, a feedback.csv file with feedback instances, and finally any csv files that may have been uploaded from the web interface. This is complementary to the -e option that exports the scenario configuration. -l or \u2013license-key provides the license key to Data Preparer; a license key is emailed to the email address provided when downloading the software. The argument in this parameter provides either the license key itself or the path to a file containing the license key. In order not to have to specify the license key every time Data Preparer runs, a file containing \u2019license\u2019 in its name, can be placed in the same folder with the executable, containing the license key. -o or \u2013override property=value This loads the scenario and overrides one or more properties in the configuration. For instance, -o product.name=p1,product.size=200 will set the product name to p1 and the number of tuples to include in the end product to 200. Note that no spaces are allowed in the property-value pairs. -p or \u2013properties filename filename is a path to a properties file that configures the wrangling process. -q or \u2013quit Terminates any running Data Preparer instances. -r or \u2013server-port port-number This changes the port on which Data Preparer runs, from the default 8182 to port-number. For instance, running with -r 8183 means that the web interface will be accessible at http://localhost:8183. -s or \u2013sources filename filename is a path to a sources file that describes where the data sources are. -w or \u2013wrangle run the wrangling process. -x or \u2013export filename filename is the path to a csv file that should be used to store the result of the wrangling process.","title":"Command parameters:"},{"location":"advanced/command-line/intro/","text":"Command Line Tool Datapreparer also has a command line version available. Prerequisite To use the command line version, you need to have Java 11 installed. Download The command line version of the tool can be downloaded here #todo! Directory Structure In the command line version, there is no function to use .scenario files. Instead, the .scenario file is unbundled and broken down into files similar to below, and these files are passed into the command line individually. When working with the command-line version of Data Preparer, we recommend that the directory structure used for the examples in the zipped archive is adopted for further projects. - scenario-name - data - data-source1.csv - data-source2.csv - data-source3.csv - data-context.csv - feedback.csv - sources.csv - wrangle1.properties - wrangle2.properties","title":"Setup"},{"location":"advanced/command-line/intro/#command-line-tool","text":"Datapreparer also has a command line version available.","title":"Command Line Tool"},{"location":"advanced/command-line/intro/#prerequisite","text":"To use the command line version, you need to have Java 11 installed.","title":"Prerequisite"},{"location":"advanced/command-line/intro/#download","text":"The command line version of the tool can be downloaded here #todo!","title":"Download"},{"location":"advanced/command-line/intro/#directory-structure","text":"In the command line version, there is no function to use .scenario files. Instead, the .scenario file is unbundled and broken down into files similar to below, and these files are passed into the command line individually. When working with the command-line version of Data Preparer, we recommend that the directory structure used for the examples in the zipped archive is adopted for further projects. - scenario-name - data - data-source1.csv - data-source2.csv - data-source3.csv - data-context.csv - feedback.csv - sources.csv - wrangle1.properties - wrangle2.properties","title":"Directory Structure"},{"location":"advanced/command-line/usages/","text":"Suggested Usages The command line can be used in different ways. One approach is to use the configuration files for the initial information for a data preparation activity, and to do most of the real work from the visual interface. For the book promotions example, made available in the directory ManualBooks, this means running the command: ./datapreparer.sh -p ManualBooks/wrangle1.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv Then most of the work is within the visual interface, from which sessions can be saved as described in the next section. Then the result can be viewed, the properties file tweaked, new sources added, etc, and the command run again. An alternative to the above would be running: ./datapreparer.sh -i ManualBooks/ManualBooks.scenario However, it is also possible to use the command line extensively in development. This might involve running the following command line, exporting the result of the wrangling process into a csv file without accessing the web interface: ./datapreparer.sh -b -i ManualBooks.scenario -x ManualBooks/result.csv Running the following command allows experimenting with a different set of wrangling properties (wrangle2.properties): ./datapreparer.sh -p ManualBooks/wrangle2.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv Commands could be run in order to test different scenario outcomes, for instance running the following in Linux, would produce two end products, e1 and e2, as results of two different tests on a system property: ./datapreparer.sh -i s1.scenario -b -r 8181 -o match.threshold=0.5 -x e1.csv & ./datapreparer.sh -i s2.scenario -b -r 8182 -o match.threshold=0.6 -x e2.csv Another example, in Windows, would be running two scenarios with the same sources and data context but with different properties, to produce two end products: datapreparer.bat -b -p ManualBooks/wrangle1.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv -r 8181 -x result1.csv datapreparer.bat -b -p ManualBooks/wrangle2.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv -r 8182 -x result2.csv You can combine the command line and user interface. The initial runs are performed using the command line and refined using the user interface. The command line is then used in production to apply the wrangling steps to the updated source files.","title":"Usages"},{"location":"advanced/command-line/usages/#suggested-usages","text":"The command line can be used in different ways. One approach is to use the configuration files for the initial information for a data preparation activity, and to do most of the real work from the visual interface. For the book promotions example, made available in the directory ManualBooks, this means running the command: ./datapreparer.sh -p ManualBooks/wrangle1.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv Then most of the work is within the visual interface, from which sessions can be saved as described in the next section. Then the result can be viewed, the properties file tweaked, new sources added, etc, and the command run again. An alternative to the above would be running: ./datapreparer.sh -i ManualBooks/ManualBooks.scenario However, it is also possible to use the command line extensively in development. This might involve running the following command line, exporting the result of the wrangling process into a csv file without accessing the web interface: ./datapreparer.sh -b -i ManualBooks.scenario -x ManualBooks/result.csv Running the following command allows experimenting with a different set of wrangling properties (wrangle2.properties): ./datapreparer.sh -p ManualBooks/wrangle2.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv Commands could be run in order to test different scenario outcomes, for instance running the following in Linux, would produce two end products, e1 and e2, as results of two different tests on a system property: ./datapreparer.sh -i s1.scenario -b -r 8181 -o match.threshold=0.5 -x e1.csv & ./datapreparer.sh -i s2.scenario -b -r 8182 -o match.threshold=0.6 -x e2.csv Another example, in Windows, would be running two scenarios with the same sources and data context but with different properties, to produce two end products: datapreparer.bat -b -p ManualBooks/wrangle1.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv -r 8181 -x result1.csv datapreparer.bat -b -p ManualBooks/wrangle2.properties -s ManualBooks/sources.csv -d ManualBooks/datacontext.csv -r 8182 -x result2.csv You can combine the command line and user interface. The initial runs are performed using the command line and refined using the user interface. The command line is then used in production to apply the wrangling steps to the updated source files.","title":"Suggested Usages"},{"location":"advanced/command-line/files/data-context/","text":"Data Context This file describes resources in the data context that should be included in the data wrangling scenario. Format of the file We have provided an example of how the contents of the file should be structured in datacontext.csv in the Manual Books example. The first 8 columns in this file are identical to the ones describing the sources : uri, tables, description, data, extensions, separator, recursive, overwrite. However, columns 9, 10, and 11 should be as follows: Columns Explaination type The data context type of the resource. It can be any of the following: example for example values, master for master data, and reference for reference data. included (optional) A comma-separated list of the fields of the imported resource to include in the created data context relationship. List items are in the form field1:field2, where field1 is one of the product field names, and field2 is a field in the resource that is imported. If none is specified here, the data context relationships will be created between fields that share the exact same name in the end product and the newly imported resource. E.g. the entry name:newfield1 , description:newfield2 means that relationships between the target schema attributes name and description, and the resource attributes newfield1 and newfield1, should be created, respectively. excluded (optional) A comma-separated list of fields of the imported resource to exclude from the data context relationship, as the column included described above. Note, a resource-level and an entity-level relationship will still be created for the imported resource.","title":"Data Context"},{"location":"advanced/command-line/files/data-context/#data-context","text":"This file describes resources in the data context that should be included in the data wrangling scenario.","title":"Data Context"},{"location":"advanced/command-line/files/data-context/#format-of-the-file","text":"We have provided an example of how the contents of the file should be structured in datacontext.csv in the Manual Books example. The first 8 columns in this file are identical to the ones describing the sources : uri, tables, description, data, extensions, separator, recursive, overwrite. However, columns 9, 10, and 11 should be as follows: Columns Explaination type The data context type of the resource. It can be any of the following: example for example values, master for master data, and reference for reference data. included (optional) A comma-separated list of the fields of the imported resource to include in the created data context relationship. List items are in the form field1:field2, where field1 is one of the product field names, and field2 is a field in the resource that is imported. If none is specified here, the data context relationships will be created between fields that share the exact same name in the end product and the newly imported resource. E.g. the entry name:newfield1 , description:newfield2 means that relationships between the target schema attributes name and description, and the resource attributes newfield1 and newfield1, should be created, respectively. excluded (optional) A comma-separated list of fields of the imported resource to exclude from the data context relationship, as the column included described above. Note, a resource-level and an entity-level relationship will still be created for the imported resource.","title":"Format of the file"},{"location":"advanced/command-line/files/feedback/","text":"Feedback This file describes the feedback instances, one feedback instance per line. Format of the file We have provided an example of how the contents of the file should be structured in feedback.csv in the Manual Books example. Headers The headers in this file should match the names and the order of the fields of the target schema, as defined in the property product.fields (see Wrangle Properties ). Columns Then, an additional column header is expected, titled relevant. Positive values in this column (any of: yes, y, 1, true, or t) indicate that the values in the feedback instance are relevant. If more than one value is filled in a CSV line, then the line is considered to be tuple-level feedback. If only one value is present, then the line is considered to be attribute-level feedback.","title":"Feedback"},{"location":"advanced/command-line/files/feedback/#feedback","text":"This file describes the feedback instances, one feedback instance per line.","title":"Feedback"},{"location":"advanced/command-line/files/feedback/#format-of-the-file","text":"We have provided an example of how the contents of the file should be structured in feedback.csv in the Manual Books example.","title":"Format of the file"},{"location":"advanced/command-line/files/feedback/#headers","text":"The headers in this file should match the names and the order of the fields of the target schema, as defined in the property product.fields (see Wrangle Properties ).","title":"Headers"},{"location":"advanced/command-line/files/feedback/#columns","text":"Then, an additional column header is expected, titled relevant. Positive values in this column (any of: yes, y, 1, true, or t) indicate that the values in the feedback instance are relevant. If more than one value is filled in a CSV line, then the line is considered to be tuple-level feedback. If only one value is present, then the line is considered to be attribute-level feedback.","title":"Columns"},{"location":"advanced/command-line/files/sources/","text":"Sources The file describing how to access the sources, passed as an argument to the -s command line option. Format of the file An example has been provided of how the contents of the file should be structured in the sources.csv file in the Manual Books example. The csv file is expected to contain exactly the following columns: Columns Explaination uri A jdbc connection string to an external database, as described in Adding Data Uri . If a uri is not specified here, then the source entry should be defined in column data. tables (optional) In case a uri is provided, a comma-separated list of the specific tables in the database schema to consider in the wrangling process. If none is specified, all tables will be considered. description (optional) A description of the resource. data A file directory, or a delimiter-separated text file. The file path can be either absolute, or relative to the location of the Data Preparer executable. The first line in each source file is expected to contain the field names. The delimiter can optionally be defined in the separator column. extensions A comma-separated list of file extensions. In case a directory is specified in the data column, we can optionally filter by extension the files to wrangle. E.g. csv, tsv. separator (optional) An explicit separator character to use when parsing a delimiterseparated text file defined in the data column. The delimiter can be a comma, a tab, a colon, or a pipe (, or \\t or : or recursive In case a file directory is specified in the data column, we defined here whether the system should also scan its subdirectories for data files to import. Default is false . overwrite When a uri is defined, we can choose whether the uri contents are retrieved and overwrite the locally cached ones every time a wrangling process starts. In case both a connection to a Postgres database and a data file are defined in the uri and data columns respectively, we can choose whether the database contents will be overwritten by the data file contents. Default is false . exclusions A comma-separated list of fields in the source and the respective wrangling steps from which they should be excluded. For instance, an entry table1.field1:repair, table2.field2:ckey, means that attribute field1 in table1 should not be repaired, and attribute field2 in table table2 should not be a candidate key. Wrangling steps that can be excluded are: match, repair, transform, ckey, ind, join. matches A comma-separated list of user-definedmatches of source fields to target schema fields, in the form: table1.field1:end_product_table:field2 . examples A comma-separated list of user-defined transformation examples of the form source value -> target value . The contents of this field should be included in double quotes, so as to ensure that parsing the csv file is not affected.","title":"Sources"},{"location":"advanced/command-line/files/sources/#sources","text":"The file describing how to access the sources, passed as an argument to the -s command line option.","title":"Sources"},{"location":"advanced/command-line/files/sources/#format-of-the-file","text":"An example has been provided of how the contents of the file should be structured in the sources.csv file in the Manual Books example. The csv file is expected to contain exactly the following columns: Columns Explaination uri A jdbc connection string to an external database, as described in Adding Data Uri . If a uri is not specified here, then the source entry should be defined in column data. tables (optional) In case a uri is provided, a comma-separated list of the specific tables in the database schema to consider in the wrangling process. If none is specified, all tables will be considered. description (optional) A description of the resource. data A file directory, or a delimiter-separated text file. The file path can be either absolute, or relative to the location of the Data Preparer executable. The first line in each source file is expected to contain the field names. The delimiter can optionally be defined in the separator column. extensions A comma-separated list of file extensions. In case a directory is specified in the data column, we can optionally filter by extension the files to wrangle. E.g. csv, tsv. separator (optional) An explicit separator character to use when parsing a delimiterseparated text file defined in the data column. The delimiter can be a comma, a tab, a colon, or a pipe (, or \\t or : or recursive In case a file directory is specified in the data column, we defined here whether the system should also scan its subdirectories for data files to import. Default is false . overwrite When a uri is defined, we can choose whether the uri contents are retrieved and overwrite the locally cached ones every time a wrangling process starts. In case both a connection to a Postgres database and a data file are defined in the uri and data columns respectively, we can choose whether the database contents will be overwritten by the data file contents. Default is false . exclusions A comma-separated list of fields in the source and the respective wrangling steps from which they should be excluded. For instance, an entry table1.field1:repair, table2.field2:ckey, means that attribute field1 in table1 should not be repaired, and attribute field2 in table table2 should not be a candidate key. Wrangling steps that can be excluded are: match, repair, transform, ckey, ind, join. matches A comma-separated list of user-definedmatches of source fields to target schema fields, in the form: table1.field1:end_product_table:field2 . examples A comma-separated list of user-defined transformation examples of the form source value -> target value . The contents of this field should be included in double quotes, so as to ensure that parsing the csv file is not affected.","title":"Format of the file"},{"location":"advanced/command-line/files/wrangle-properties/","text":"Wrangle Properties In the command line version, the wrangle properties in the control panel tab in the web browser detailed in the Steering Section can also be specified in a properties file, eg. wrangle.properties and passed to Data Preparer using the -p command line option. Note: We use .properties as our naming convention, but you are free to use whatever name you like. Format of the file We have provided an example of how the contents of the file should be structured in wrangle1.properties and wrangle2.properties in the Manual Books example. The properties in this file are expected to be as described below: General Settings workflow : The system components that will participate in the wrangling process. This is a comma-separated list, containing any of the following: match, repair, transform, profile, map, select. Note that some of these components require the output of others, as follows: repair requires match; transform requires match; map requires match, profile; and select requires map, match, profile. Note: A required component will be executed, even if it is omitted here. For instance, defining a workflow as map, select, will trigger execution of the workflow match, profile, map, select, because match and profile are required by map. Note: The default workflow includes all components: match, repair, transform, profile, map, select. workflow.incremental : Setting the value of this property to no, means that when a new wrangling process is executed, all components will be executed. Setting it to yes, means that when a new wrangling process is executed, any existing results will be reused i.e. components whose configuration hasn\u2019t changed since the last run will not run again. The default value is yes. product.name : The end product table name. product.fields : A comma-separated list of the end product fields and their exclusions. Exclusions from wrangling steps can be defined here as colon-separated sets following the names. Wrangling steps that can be excluded are: repair, transform, join. For instance, defining product fields as name:repair:transform, desc:transform:join means that the end product will comprise two fields, name and desc, while source attributes matching to name will not be repaired and will not be transformed, and source attributes matching to desc will not be joined to any other attributes. Note that these are slightly different from the workflow steps defined in the workflow property above. product.uri : This describes the connection to an external database, where the end product should be stored, so that it remains available even after Data Preparer exits. The jdbc uri must also contain the login credentials, as described in Adding Data Uri . The default value is an empty string, which means the end product will be stored in memory for as long as the application runs. Note that any pre-existing data in this uri will be overwritten. filters and criteria : These two properties describe the User Context , i.e. your preferences over the characteristics of the end product. The filters are hard constraints on the end product, meaning that the end product should satisfy these constraints. Criteria and filters can be of any of the following types: Standalone, that require no parameters for their evaluation. - completeness. The fraction of the values that are null or empty. - tuple_completeness. The fraction of the tuples with non-null, non-empty values. Can only be evaluated on a table. - numeric. The fraction of the values that are numbers. - datetime. The fraction of the values that represent a date or a time. - is_email. The fraction of the values that are emails. - is_uri. The fraction of the values that are uris. - contain_numbers. The fraction of values that contain any number. - contain_letters. The fraction of values that contain any letter. - contain_punctuation. The fraction of values that contain punctuation characters, i.e. characters that are not letters, or numbers, or whitespace. - contain_email. The fraction of the values that contain emails. - contain_uri. The fraction of the values that contain uris. Data context-related, that require the presence of data context. - data_context_completeness. The fraction of the values that are included in the Master Data or the Example values. - data_context_consistency. The fraction of the values that are included in the Reference Data or the Example values. Feedback-related, that require the presence of user feedback. - relevance. The fraction of the values that the user has marked as relevant. An example of a filter is completeness:product.name:no, which means that the attribute name of the end product table product must be complete. The no in the end is optional, and means that the filter is not inverted. Criteria are defined in the same way, with the addition of weights. An example definition of criteria is: completeness:product:0.5:yes, relevance:product.name:0.5:no. Match Transducer match.strategy : This can be any of: schema, instances, schema+instances. When the match strategy is set to schema, only attribute names will be compared for similarity. When set to instances, only attribute instances will be compared for similarity. When set to schema+instances, both attribute names and instances will be compared for similarity. The default value is schema. match.threshold : Matches scoring below this similarity threshold will be discarded. The default value is 0.4. match.instances.max : The number of instances to compare when evaluating similarity between two attributes. The default value is 1000. match.exclude : A colon-separated set of matches to be excluded from the wrangling process. Matches are declared between any source attribute and any target schema attributes. Source attributes are of the form schema.table.attribute, while target schema attributes are of the form table.attribute. An example of a match is: schemaA.tableA.attrA~end_product.attrX. By default, no matches are excluded. Repair Transducer repair.apply_rules : When set to yes, the discovered repair rules will modify the sources. When set to no, you can verify the changes and choose whether to discard or apply them. In case of sources in external databases, their contents will not be modified. Only their copies that are imported in Data Preparer will be modified. The default value is no. repair.rule.exclude : A comma-separated set of repair rules to exclude. The rules are defined in the form schemaA.tableA.attrA -> schemaB.tableB.attrB, where the values of attribute attrA determine the values of attribute attrB. As such, attrA is the determinant attribute, and attrB the dependent attribute. By default, no repair rules are excluded. Transform Transducer transform.examples_minpercent : The minimum percentage of the source used to create an example pair. The default value is 10, which means that a minimum of 10 percent of the source values will be used to create an example pair. transform.examples_minsize : The minimum number of source values used to create an example pair. The default value is 10, which means that a minimum of 10 distinct source values will be used to create an example pair. transform.fail_limit : The number of example candidates that are allowed to fail before the entire set is rejected. The default value is 2, which means that if 2 or more of the discovered example candidates fail, the whole example set for these attributes will be rejected. transform.source_limit : The amount of data considered from the source. The default value is 0, which means that all data from each source will be considered. transform.apply_transformations : When set to yes, the learned transformation examples will result in transformations that will modify the sources. When set to no, you can verify the changes and choose whether to discard or apply them. In case of sources in external databases, their contents will not be modified. Only their copies that are imported in Data Preparer will be modified. The default value is no. transform.example.exclude : A comma-separated set of transformation examples to be excluded from the wrangling process. The examples are of form: source value -> target value . When values contain commas, the whole examples should be included in double quotes. Profile Transducer profile.detect_ckeys : Detect candidate keys in the available sources. The default value is yes. profile.ckey.size.max : The maximum number of attributes comprising a candidate key. The default value is 1. profile.ckey.exclude : A comma-separated set of candidate keys to exclude from the wrangling process. Candidate keys can containmore than one attribute, and they are expressed as colon-separated lists of these attributes. An example of a candidate key consisting of two attributes is schemaA.tableA.attrA:schemaB.tableB.attrB. profile.detect_inds : Detect inclusion dependencies in the available sources. The default value is yes. profile.ind.overlap.threshold : Inclusion dependencies with an overlap below this threshold will be discarded. This means that only attributes with an overlap above this threshold will be considered for a full outer join. The default value is 0.5. profile.ind.matches_only : Only calculate inclusion dependencies in which at least one attribute matches the target schema. The default value is no. profile.ind.ckeys_only : Only calculate inclusion dependencies in which at least one attribute is a candidate key. The default value is yes. profile.ind.exclude : A comma-separated set of inclusion dependencies to exclude from the wrangling process. An example of an inclusion dependency is: schemaA.tableA.attrA < schemaB.tableB.attrB, which means that the inclusion dependency between attributes attrA and attrB will be ignored. Mapping Transducer mapping.output.top_k : The maximum number of candidatemappings that will be produced by the mapping generator. The default value is 100. mapping.size.max : The maximum number of relations in each candidate mapping. The default value is 10. mapping.exclude.generation : A comma-separated set of candidate mappings to exclude from generation. This means that the candidate mappings listed here will not be generated. An example of a candidate mapping to be excluded is: schemaA.tableA [Join attrA = attrB] schemaB.tableB, which joins tables tableA and tableB, which are in sources sourceA and sourceB, respectively, on attributes attrA and attrB. criteria.limit : Limit the number of tuples when evaluating criteria. The default value is 0, for unlimited. product.size : The number of tuples to include in the end product. The default value is 100. product.full_mappings : When set to no, end product will contain exactly as many tuples as defined here. When set to yes, the mapping selection component will keep selecting full mappings until it goes above the number of tuples defined here. The default value is no. Select Transducer selection.exclude.mapping : A comma-separated set of candidatemappings to exclude from selection. This means that regardless of whether these mappings have been generated by the mapping generation component, no tuples will be selected from them to populate the end product. An example of a candidate mapping is: schemaA.tableA[Join attrA = attrB] schemaB.tableB, which joins tables tableA and tableB, which are in sources sourceA and sourceB, respectively, on attributes attrA and attrB.","title":"Wrangle Properties"},{"location":"advanced/command-line/files/wrangle-properties/#wrangle-properties","text":"In the command line version, the wrangle properties in the control panel tab in the web browser detailed in the Steering Section can also be specified in a properties file, eg. wrangle.properties and passed to Data Preparer using the -p command line option. Note: We use .properties as our naming convention, but you are free to use whatever name you like.","title":"Wrangle Properties"},{"location":"advanced/command-line/files/wrangle-properties/#format-of-the-file","text":"We have provided an example of how the contents of the file should be structured in wrangle1.properties and wrangle2.properties in the Manual Books example. The properties in this file are expected to be as described below:","title":"Format of the file"},{"location":"advanced/command-line/files/wrangle-properties/#general-settings","text":"workflow : The system components that will participate in the wrangling process. This is a comma-separated list, containing any of the following: match, repair, transform, profile, map, select. Note that some of these components require the output of others, as follows: repair requires match; transform requires match; map requires match, profile; and select requires map, match, profile. Note: A required component will be executed, even if it is omitted here. For instance, defining a workflow as map, select, will trigger execution of the workflow match, profile, map, select, because match and profile are required by map. Note: The default workflow includes all components: match, repair, transform, profile, map, select. workflow.incremental : Setting the value of this property to no, means that when a new wrangling process is executed, all components will be executed. Setting it to yes, means that when a new wrangling process is executed, any existing results will be reused i.e. components whose configuration hasn\u2019t changed since the last run will not run again. The default value is yes. product.name : The end product table name. product.fields : A comma-separated list of the end product fields and their exclusions. Exclusions from wrangling steps can be defined here as colon-separated sets following the names. Wrangling steps that can be excluded are: repair, transform, join. For instance, defining product fields as name:repair:transform, desc:transform:join means that the end product will comprise two fields, name and desc, while source attributes matching to name will not be repaired and will not be transformed, and source attributes matching to desc will not be joined to any other attributes. Note that these are slightly different from the workflow steps defined in the workflow property above. product.uri : This describes the connection to an external database, where the end product should be stored, so that it remains available even after Data Preparer exits. The jdbc uri must also contain the login credentials, as described in Adding Data Uri . The default value is an empty string, which means the end product will be stored in memory for as long as the application runs. Note that any pre-existing data in this uri will be overwritten. filters and criteria : These two properties describe the User Context , i.e. your preferences over the characteristics of the end product. The filters are hard constraints on the end product, meaning that the end product should satisfy these constraints. Criteria and filters can be of any of the following types: Standalone, that require no parameters for their evaluation. - completeness. The fraction of the values that are null or empty. - tuple_completeness. The fraction of the tuples with non-null, non-empty values. Can only be evaluated on a table. - numeric. The fraction of the values that are numbers. - datetime. The fraction of the values that represent a date or a time. - is_email. The fraction of the values that are emails. - is_uri. The fraction of the values that are uris. - contain_numbers. The fraction of values that contain any number. - contain_letters. The fraction of values that contain any letter. - contain_punctuation. The fraction of values that contain punctuation characters, i.e. characters that are not letters, or numbers, or whitespace. - contain_email. The fraction of the values that contain emails. - contain_uri. The fraction of the values that contain uris. Data context-related, that require the presence of data context. - data_context_completeness. The fraction of the values that are included in the Master Data or the Example values. - data_context_consistency. The fraction of the values that are included in the Reference Data or the Example values. Feedback-related, that require the presence of user feedback. - relevance. The fraction of the values that the user has marked as relevant. An example of a filter is completeness:product.name:no, which means that the attribute name of the end product table product must be complete. The no in the end is optional, and means that the filter is not inverted. Criteria are defined in the same way, with the addition of weights. An example definition of criteria is: completeness:product:0.5:yes, relevance:product.name:0.5:no.","title":"General Settings"},{"location":"advanced/command-line/files/wrangle-properties/#match-transducer","text":"match.strategy : This can be any of: schema, instances, schema+instances. When the match strategy is set to schema, only attribute names will be compared for similarity. When set to instances, only attribute instances will be compared for similarity. When set to schema+instances, both attribute names and instances will be compared for similarity. The default value is schema. match.threshold : Matches scoring below this similarity threshold will be discarded. The default value is 0.4. match.instances.max : The number of instances to compare when evaluating similarity between two attributes. The default value is 1000. match.exclude : A colon-separated set of matches to be excluded from the wrangling process. Matches are declared between any source attribute and any target schema attributes. Source attributes are of the form schema.table.attribute, while target schema attributes are of the form table.attribute. An example of a match is: schemaA.tableA.attrA~end_product.attrX. By default, no matches are excluded.","title":"Match Transducer"},{"location":"advanced/command-line/files/wrangle-properties/#repair-transducer","text":"repair.apply_rules : When set to yes, the discovered repair rules will modify the sources. When set to no, you can verify the changes and choose whether to discard or apply them. In case of sources in external databases, their contents will not be modified. Only their copies that are imported in Data Preparer will be modified. The default value is no. repair.rule.exclude : A comma-separated set of repair rules to exclude. The rules are defined in the form schemaA.tableA.attrA -> schemaB.tableB.attrB, where the values of attribute attrA determine the values of attribute attrB. As such, attrA is the determinant attribute, and attrB the dependent attribute. By default, no repair rules are excluded.","title":"Repair Transducer"},{"location":"advanced/command-line/files/wrangle-properties/#transform-transducer","text":"transform.examples_minpercent : The minimum percentage of the source used to create an example pair. The default value is 10, which means that a minimum of 10 percent of the source values will be used to create an example pair. transform.examples_minsize : The minimum number of source values used to create an example pair. The default value is 10, which means that a minimum of 10 distinct source values will be used to create an example pair. transform.fail_limit : The number of example candidates that are allowed to fail before the entire set is rejected. The default value is 2, which means that if 2 or more of the discovered example candidates fail, the whole example set for these attributes will be rejected. transform.source_limit : The amount of data considered from the source. The default value is 0, which means that all data from each source will be considered. transform.apply_transformations : When set to yes, the learned transformation examples will result in transformations that will modify the sources. When set to no, you can verify the changes and choose whether to discard or apply them. In case of sources in external databases, their contents will not be modified. Only their copies that are imported in Data Preparer will be modified. The default value is no. transform.example.exclude : A comma-separated set of transformation examples to be excluded from the wrangling process. The examples are of form: source value -> target value . When values contain commas, the whole examples should be included in double quotes.","title":"Transform Transducer"},{"location":"advanced/command-line/files/wrangle-properties/#profile-transducer","text":"profile.detect_ckeys : Detect candidate keys in the available sources. The default value is yes. profile.ckey.size.max : The maximum number of attributes comprising a candidate key. The default value is 1. profile.ckey.exclude : A comma-separated set of candidate keys to exclude from the wrangling process. Candidate keys can containmore than one attribute, and they are expressed as colon-separated lists of these attributes. An example of a candidate key consisting of two attributes is schemaA.tableA.attrA:schemaB.tableB.attrB. profile.detect_inds : Detect inclusion dependencies in the available sources. The default value is yes. profile.ind.overlap.threshold : Inclusion dependencies with an overlap below this threshold will be discarded. This means that only attributes with an overlap above this threshold will be considered for a full outer join. The default value is 0.5. profile.ind.matches_only : Only calculate inclusion dependencies in which at least one attribute matches the target schema. The default value is no. profile.ind.ckeys_only : Only calculate inclusion dependencies in which at least one attribute is a candidate key. The default value is yes. profile.ind.exclude : A comma-separated set of inclusion dependencies to exclude from the wrangling process. An example of an inclusion dependency is: schemaA.tableA.attrA < schemaB.tableB.attrB, which means that the inclusion dependency between attributes attrA and attrB will be ignored.","title":"Profile Transducer"},{"location":"advanced/command-line/files/wrangle-properties/#mapping-transducer","text":"mapping.output.top_k : The maximum number of candidatemappings that will be produced by the mapping generator. The default value is 100. mapping.size.max : The maximum number of relations in each candidate mapping. The default value is 10. mapping.exclude.generation : A comma-separated set of candidate mappings to exclude from generation. This means that the candidate mappings listed here will not be generated. An example of a candidate mapping to be excluded is: schemaA.tableA [Join attrA = attrB] schemaB.tableB, which joins tables tableA and tableB, which are in sources sourceA and sourceB, respectively, on attributes attrA and attrB. criteria.limit : Limit the number of tuples when evaluating criteria. The default value is 0, for unlimited. product.size : The number of tuples to include in the end product. The default value is 100. product.full_mappings : When set to no, end product will contain exactly as many tuples as defined here. When set to yes, the mapping selection component will keep selecting full mappings until it goes above the number of tuples defined here. The default value is no.","title":"Mapping Transducer"},{"location":"advanced/command-line/files/wrangle-properties/#select-transducer","text":"selection.exclude.mapping : A comma-separated set of candidatemappings to exclude from selection. This means that regardless of whether these mappings have been generated by the mapping generation component, no tuples will be selected from them to populate the end product. An example of a candidate mapping is: schemaA.tableA[Join attrA = attrB] schemaB.tableB, which joins tables tableA and tableB, which are in sources sourceA and sourceB, respectively, on attributes attrA and attrB.","title":"Select Transducer"},{"location":"guides/data-prep-process/","text":"Data Preperation Process Motivation There is no widely accepted data preparation process, and Data Preparer doesn't impose or even imply a particular process. However, there is inherent uncertainty associated with data preparation. Individual sources' properties and their relationships are not fully understood up-front. This suggests that an iterative process will be standard. The Datapreparer Workflow Data Preparer might be well suited to an iterative process, as automation enables rapid re-wrangling with additional sources or evidence. Here is a possible mode of operation: Iterate the following steps: (Re)Define the target: The purpose of the wrangling process is to populate the target. This purpose needs to be made explicit through a definition for the target. This may then be refined as problems with the proposal or opportunities from the data emerge. Obtain source and data context data sets: Identify data sets that can be used to populate the target and data context that can be aligned with as many target attributes as possible. Wrangle using Data Preparer: Running Data Preparer will produce a result based on the current inputs. Review the results: If suitable, then exit the iteration. Refine the process: Review explanation of how results were produced: identifying the most appropriate next step requires a good understanding of what is possible with the available data and how Data Preparer is currently wrangling the data. This means reviewing the result and how it was obtained in some detail. Revise the user context: If relevant data is present, but is not being selected, refine the user context to provide a better result. Identify new sources or data context to add: if there is insufficient data or insufficient evidence to enable it to be wrangled effectively, consider how to fill these gaps. Revise how Data Preparer is wrangling the data: Change the control parameters, or engage in more detailed steering by changing the results of individual steps or refining how individual sources participate in wrangling","title":"Data Preparation Process"},{"location":"guides/data-prep-process/#data-preperation-process","text":"","title":"Data Preperation Process"},{"location":"guides/data-prep-process/#motivation","text":"There is no widely accepted data preparation process, and Data Preparer doesn't impose or even imply a particular process. However, there is inherent uncertainty associated with data preparation. Individual sources' properties and their relationships are not fully understood up-front. This suggests that an iterative process will be standard.","title":"Motivation"},{"location":"guides/data-prep-process/#the-datapreparer-workflow","text":"Data Preparer might be well suited to an iterative process, as automation enables rapid re-wrangling with additional sources or evidence. Here is a possible mode of operation:","title":"The Datapreparer Workflow"},{"location":"guides/data-prep-process/#iterate-the-following-steps","text":"(Re)Define the target: The purpose of the wrangling process is to populate the target. This purpose needs to be made explicit through a definition for the target. This may then be refined as problems with the proposal or opportunities from the data emerge. Obtain source and data context data sets: Identify data sets that can be used to populate the target and data context that can be aligned with as many target attributes as possible. Wrangle using Data Preparer: Running Data Preparer will produce a result based on the current inputs. Review the results: If suitable, then exit the iteration. Refine the process: Review explanation of how results were produced: identifying the most appropriate next step requires a good understanding of what is possible with the available data and how Data Preparer is currently wrangling the data. This means reviewing the result and how it was obtained in some detail. Revise the user context: If relevant data is present, but is not being selected, refine the user context to provide a better result. Identify new sources or data context to add: if there is insufficient data or insufficient evidence to enable it to be wrangled effectively, consider how to fill these gaps. Revise how Data Preparer is wrangling the data: Change the control parameters, or engage in more detailed steering by changing the results of individual steps or refining how individual sources participate in wrangling","title":"Iterate the following steps:"},{"location":"guides/scenario/","text":"Scenarios What is a scenario A scenario is datapreparer's way of representing a project. In datapreparer, you start a wrangle project by first creating a scenario through the scenario wizard. Datapreparer allows you to save your progress and export your project into .scenario files to be shared with other collaborators or to continue your work at another time. Create a scenario through the Scenario Wizard First, click on the big plus button on the bottom right of the screen. Click on the start wizard button to launch the scenario wizard. The wizard will guide you through every step of the process, you can find detailed information on the bottom right at each step. Steps Create your target schema. Set how you want your end product data to look like by setting your desired table fields. Add your data sources Add your data sources, csv files, json files, connect your databases etc. Full details in Adding Data section. Set the data context Add additional information about your target to improve the wrangle process by adding reference data, master data and example data. Full details in Data Context section. Set the user context Set your requirements on the wrangling process by setting criteria or filters. Full details in User Context section. Wrangle! Click on the >> button to wrangle the data and produce an end product. Feedback Give your own input on the end product, manually select which rows are relevant/non-relevant. Full details in Refining Solutions section. Repeat step 5-7 until satisfied Scenario controls The datapreparer contains some features for controlling scenarios, which are located under the home tab. Importing scenario You can import a .scenario file from a previous project or from a collaborator. Exporting scenario You can export a wrangle project into a .scenario file to work on later or share to another collaborator. Clear scenario You can clear a scenario that you don't want to work on anymore. Note: Once a scenario is cleared, you can't get the data back, so think twice before clearing a scenario.","title":"Intro to Scenarios"},{"location":"guides/scenario/#scenarios","text":"","title":"Scenarios"},{"location":"guides/scenario/#what-is-a-scenario","text":"A scenario is datapreparer's way of representing a project. In datapreparer, you start a wrangle project by first creating a scenario through the scenario wizard. Datapreparer allows you to save your progress and export your project into .scenario files to be shared with other collaborators or to continue your work at another time.","title":"What is a scenario"},{"location":"guides/scenario/#create-a-scenario-through-the-scenario-wizard","text":"First, click on the big plus button on the bottom right of the screen. Click on the start wizard button to launch the scenario wizard. The wizard will guide you through every step of the process, you can find detailed information on the bottom right at each step.","title":"Create a scenario through the Scenario Wizard"},{"location":"guides/scenario/#steps","text":"Create your target schema. Set how you want your end product data to look like by setting your desired table fields. Add your data sources Add your data sources, csv files, json files, connect your databases etc. Full details in Adding Data section. Set the data context Add additional information about your target to improve the wrangle process by adding reference data, master data and example data. Full details in Data Context section. Set the user context Set your requirements on the wrangling process by setting criteria or filters. Full details in User Context section. Wrangle! Click on the >> button to wrangle the data and produce an end product. Feedback Give your own input on the end product, manually select which rows are relevant/non-relevant. Full details in Refining Solutions section. Repeat step 5-7 until satisfied","title":"Steps"},{"location":"guides/scenario/#scenario-controls","text":"The datapreparer contains some features for controlling scenarios, which are located under the home tab.","title":"Scenario controls"},{"location":"guides/scenario/#importing-scenario","text":"You can import a .scenario file from a previous project or from a collaborator.","title":"Importing scenario"},{"location":"guides/scenario/#exporting-scenario","text":"You can export a wrangle project into a .scenario file to work on later or share to another collaborator.","title":"Exporting scenario"},{"location":"guides/scenario/#clear-scenario","text":"You can clear a scenario that you don't want to work on anymore. Note: Once a scenario is cleared, you can't get the data back, so think twice before clearing a scenario.","title":"Clear scenario"},{"location":"guides/viewing-data/","text":"ADDING EVIDENCE TO INFORM DATA PREPARATION In data preparation, a variety of data transformation and cleaning operations are available, and the application of each such operation to each source or subset of sources needs to be informed by evidence. In Data Preparer, data context is a type of evidence that provides additional information about the target. This one type of evidence is used to inform decisions in several data preparation steps. You can modify the data context at the bottom of the page, by navigating to data context from the top bar menu. Resources can be labeled as the following three types: Reference data : The complete list of all the data in a domain. For example, this could be a complete list of town names or zip codes. The correct result of a wrangling process is unlikely to contain all these values, but all correct results should correspond to the data in the reference data. Master data : The complete list of the relevant data of an application. As an example, this could be all the suppliers with which an organisation works \u2013 where there is a complete such list, it is master data. Example data : A collection of representative values. These values need not to be expected results of the wrangling process (e.g. they could be made up, or they could be from an earlier wrangling activity). In the data context tab, the submenu options to upload reference , upload master and upload examples allow the user to identify csv files in which data context data is found. Attribute names in those files that correspond to those in the target are assumed to be related, though such relationships can be added (using add relationship in the submenu) or removed (using the trash icon beside the relevant attribute). Usages The additional evidence provided in data context facilitates: Refining existing tasks For example, let's assume author names are stored in the column with name text . Without data context, the relationship between text in the source and author in the target, is missed by Data Preparer. However, by providing representative values for the author attribute of the target in the data context, Data Preparer is able to identify the association between the two columns. additional data preparation tasks For example, repairing the data set: missing values for rating in Riverside Books, can be filled in from the data context. Reformat the data set: the format used for the isbn attribute in Riverside Books, is inconsistent with that in the data context; the isbn data in Riverside Books is reformatted to use the representation in the data context (i.e., without the ISBN prefix).","title":"Data Context"},{"location":"guides/viewing-data/#adding-evidence-to-inform-data-preparation","text":"In data preparation, a variety of data transformation and cleaning operations are available, and the application of each such operation to each source or subset of sources needs to be informed by evidence. In Data Preparer, data context is a type of evidence that provides additional information about the target. This one type of evidence is used to inform decisions in several data preparation steps. You can modify the data context at the bottom of the page, by navigating to data context from the top bar menu. Resources can be labeled as the following three types: Reference data : The complete list of all the data in a domain. For example, this could be a complete list of town names or zip codes. The correct result of a wrangling process is unlikely to contain all these values, but all correct results should correspond to the data in the reference data. Master data : The complete list of the relevant data of an application. As an example, this could be all the suppliers with which an organisation works \u2013 where there is a complete such list, it is master data. Example data : A collection of representative values. These values need not to be expected results of the wrangling process (e.g. they could be made up, or they could be from an earlier wrangling activity). In the data context tab, the submenu options to upload reference , upload master and upload examples allow the user to identify csv files in which data context data is found. Attribute names in those files that correspond to those in the target are assumed to be related, though such relationships can be added (using add relationship in the submenu) or removed (using the trash icon beside the relevant attribute).","title":"ADDING EVIDENCE TO INFORM DATA PREPARATION"},{"location":"guides/viewing-data/#usages","text":"The additional evidence provided in data context facilitates: Refining existing tasks For example, let's assume author names are stored in the column with name text . Without data context, the relationship between text in the source and author in the target, is missed by Data Preparer. However, by providing representative values for the author attribute of the target in the data context, Data Preparer is able to identify the association between the two columns. additional data preparation tasks For example, repairing the data set: missing values for rating in Riverside Books, can be filled in from the data context. Reformat the data set: the format used for the isbn attribute in Riverside Books, is inconsistent with that in the data context; the isbn data in Riverside Books is reformatted to use the representation in the data context (i.e., without the ISBN prefix).","title":"Usages"},{"location":"guides/adding-data/CSV/","text":"Importing CSV data The easiest way to import data into datapreparer is to import CSV files. Prerequisites Datapreparer up and running Some CSV files you would like to import Importing the data Go to the sources tab Click upload sources . After this, you should get a prompt to choose a file. Press upload After this step, you should see the new data source. It should look something like this, You have now successfully imported data from a CSV file. From here, you can now explore and wrangle the data.","title":"From CSV files"},{"location":"guides/adding-data/CSV/#importing-csv-data","text":"The easiest way to import data into datapreparer is to import CSV files.","title":"Importing CSV data"},{"location":"guides/adding-data/CSV/#prerequisites","text":"Datapreparer up and running Some CSV files you would like to import","title":"Prerequisites"},{"location":"guides/adding-data/CSV/#importing-the-data","text":"Go to the sources tab Click upload sources . After this, you should get a prompt to choose a file. Press upload After this step, you should see the new data source. It should look something like this, You have now successfully imported data from a CSV file. From here, you can now explore and wrangle the data.","title":"Importing the data"},{"location":"guides/adding-data/JSON/","text":"JSON data support Data Preparer supports importing JSON format data as well. Importing the data Go to the sources tab Click upload sources . After this, you should get a prompt to choose a file. Press upload After this step, you should see the new data source. It should look something like this, You have now successfully imported data from a JSON file. From here, you can now explore and wrangle the data. Supported structures: With attribute names: [ {\"title\": \"THE SUN ALSO RISES\", \"author\": \"ERNEST HEMINGWAY\"}, {\"title\": \"Don Quixote\", \"author\": \"Miguel de Cervantes\"}, ] or { \"data\": [ {\"title\": \"THE SUN ALSO RISES\", \"author\": \"ERNEST HEMINGWAY\"}, {\"title\": \"Don Quixote\", \"author\": \"Miguel de Cervantes\"}, ] } Without attribute names: [ [\"THE SUN ALSO RISES\",\"ERNEST HEMINGWAY\" ], [\"Don Quixote\",\"Miguel de Cervantes\" ] ] or { \"data\":[ [\"THE SUN ALSO RISES\", \"ERNEST HEMINGWAY\"], [\"Don Quixote\", \"Miguel de Cervantes\"], ] }","title":"From JSON files"},{"location":"guides/adding-data/JSON/#json-data-support","text":"Data Preparer supports importing JSON format data as well.","title":"JSON data support"},{"location":"guides/adding-data/JSON/#importing-the-data","text":"Go to the sources tab Click upload sources . After this, you should get a prompt to choose a file. Press upload After this step, you should see the new data source. It should look something like this, You have now successfully imported data from a JSON file. From here, you can now explore and wrangle the data.","title":"Importing the data"},{"location":"guides/adding-data/JSON/#supported-structures","text":"","title":"Supported structures:"},{"location":"guides/adding-data/JSON/#with-attribute-names","text":"[ {\"title\": \"THE SUN ALSO RISES\", \"author\": \"ERNEST HEMINGWAY\"}, {\"title\": \"Don Quixote\", \"author\": \"Miguel de Cervantes\"}, ] or { \"data\": [ {\"title\": \"THE SUN ALSO RISES\", \"author\": \"ERNEST HEMINGWAY\"}, {\"title\": \"Don Quixote\", \"author\": \"Miguel de Cervantes\"}, ] }","title":"With attribute names:"},{"location":"guides/adding-data/JSON/#without-attribute-names","text":"[ [\"THE SUN ALSO RISES\",\"ERNEST HEMINGWAY\" ], [\"Don Quixote\",\"Miguel de Cervantes\" ] ] or { \"data\":[ [\"THE SUN ALSO RISES\", \"ERNEST HEMINGWAY\"], [\"Don Quixote\", \"Miguel de Cervantes\"], ] }","title":"Without attribute names:"},{"location":"guides/adding-data/external-uri/","text":"Importing data from the web If your data source is in a web server or the cloud, you can import them through the add URI feature. Importing the data Go to the sources tab and click on the add URI option. Click on the add URI option. After this, you should get a prompt to enter a URI for your data source. Note: Enabling the always read option will force the source contents to be read ahead of every wrangling process. If this option is not enabled, the source contents will be retrieved once and cached locally. Click on add source . You have now successfully imported data from the web. From here, you can now explore and wrangle the data. Supported URI types CSV files JSON files Databases","title":"From the web"},{"location":"guides/adding-data/external-uri/#importing-data-from-the-web","text":"If your data source is in a web server or the cloud, you can import them through the add URI feature.","title":"Importing data from the web"},{"location":"guides/adding-data/external-uri/#importing-the-data","text":"Go to the sources tab and click on the add URI option. Click on the add URI option. After this, you should get a prompt to enter a URI for your data source. Note: Enabling the always read option will force the source contents to be read ahead of every wrangling process. If this option is not enabled, the source contents will be retrieved once and cached locally. Click on add source . You have now successfully imported data from the web. From here, you can now explore and wrangle the data.","title":"Importing the data"},{"location":"guides/adding-data/external-uri/#supported-uri-types","text":"CSV files JSON files Databases","title":"Supported URI types"},{"location":"guides/adding-data/intro/","text":"Adding Data Data sources can be made available to Data Preparer as: Delimiter-separated files, where the delimiter is a comma, a tab, a semicolon, or a pipe character. Excel files. By connecting to relational databases. In each case, it is advised that you take a moment to verify that the data is imported correctly. Although extra effort has been put into importing source data as robust as possible, edge cases may surface related to encodings, uncommon data types, or system-specific settings.","title":"Overview"},{"location":"guides/adding-data/intro/#adding-data","text":"Data sources can be made available to Data Preparer as: Delimiter-separated files, where the delimiter is a comma, a tab, a semicolon, or a pipe character. Excel files. By connecting to relational databases. In each case, it is advised that you take a moment to verify that the data is imported correctly. Although extra effort has been put into importing source data as robust as possible, edge cases may surface related to encodings, uncommon data types, or system-specific settings.","title":"Adding Data"},{"location":"guides/adding-data/local-DB/","text":"Reading data from an external database You can also add data by connecting your existing databases to the datapreparer. Supported Databases Datapreparer supports: DB2 (version 11.5 or newer) Informix(versions 12.10, 14.10) MariaDB (version 10.4 or newer) MySQL (version 5.6 or newer) PostgreSQL (version 8.2 or newer) Presto (version 0.234.1 or newer) Snowflake (driver version 3.12.3) SQL Server (version 2017 or newer) Supported Jdbc Connection Strings Note: The schema to import needs to be defined where available. Jdbc connection strings are in a single line and should be in the following form: DB2.jdbc:db2://<\\host>:<\\port>/<\\database>:currentSchema=<\\schema>; user=<\\user>; password=<\\password>; Informix.jdbc:informix-sqli://<\\host>:<\\port>/<\\database>:user=<\\user>;password=<\\password> MariaDB.jdbc:mariadb://<\\host>:<\\port>/<\\database>?user=<\\user> &password=<\\password> MySQL.jdbc:mysql://<\\host>:<\\port>/<\\database>?user=<\\user>&password=<\\password> PostgreSQL.jdbc:postgresql://<\\host>:<\\port>/<\\database>?currentSchema=<\\schema> &user=<\\user>&password=<\\password> Presto.jdbc:presto://<\\host>:<\\port>/<\\catalog>/<\\schema>?user=<\\user>&password=<\\password> Snowflake. jdbc:snowflake:// .snowflakecomputing.com/?db= &schema= &user= &password= SQL Server.jdbc:sqlserver://<\\host>:<\\port>;database=<\\database>; user=<\\user>;password=<\\password>;","title":"From local databases"},{"location":"guides/adding-data/local-DB/#reading-data-from-an-external-database","text":"You can also add data by connecting your existing databases to the datapreparer.","title":"Reading data from an external database"},{"location":"guides/adding-data/local-DB/#supported-databases","text":"Datapreparer supports: DB2 (version 11.5 or newer) Informix(versions 12.10, 14.10) MariaDB (version 10.4 or newer) MySQL (version 5.6 or newer) PostgreSQL (version 8.2 or newer) Presto (version 0.234.1 or newer) Snowflake (driver version 3.12.3) SQL Server (version 2017 or newer)","title":"Supported Databases"},{"location":"guides/adding-data/local-DB/#supported-jdbc-connection-strings","text":"Note: The schema to import needs to be defined where available. Jdbc connection strings are in a single line and should be in the following form: DB2.jdbc:db2://<\\host>:<\\port>/<\\database>:currentSchema=<\\schema>; user=<\\user>; password=<\\password>; Informix.jdbc:informix-sqli://<\\host>:<\\port>/<\\database>:user=<\\user>;password=<\\password> MariaDB.jdbc:mariadb://<\\host>:<\\port>/<\\database>?user=<\\user> &password=<\\password> MySQL.jdbc:mysql://<\\host>:<\\port>/<\\database>?user=<\\user>&password=<\\password> PostgreSQL.jdbc:postgresql://<\\host>:<\\port>/<\\database>?currentSchema=<\\schema> &user=<\\user>&password=<\\password> Presto.jdbc:presto://<\\host>:<\\port>/<\\catalog>/<\\schema>?user=<\\user>&password=<\\password> Snowflake. jdbc:snowflake:// .snowflakecomputing.com/?db= &schema= &user= &password= SQL Server.jdbc:sqlserver://<\\host>:<\\port>;database=<\\database>; user=<\\user>;password=<\\password>;","title":"Supported Jdbc Connection Strings"},{"location":"guides/refining-solutions/feedback/","text":"Introduction Data Preparer systematically explores a space of integration and cleaning options but can certainly make decisions that are not the ones the user would prefer. As a result, Data Preparer allows the user to refine how the data wrangle should happen, either by providing feedback on the results of the data preparation task or by changing parameters that control how Data Preparer behaves, or annotate the constructs that can contribute to a solution. How to use feedback By clicking on end product after wrangling the data, you will be able to submit feedback. At the bottom of the page, you can select either cells (individual attribute values) or tuples (complete rows), then by clicking on the entries in the table, you will mark the cells/rows with the selected attribute (green - relevant, red - non-relevant). Clicking Submit feedback saves your choices. If you are not happy with a change, scroll to the feedback section and remove it.","title":"Feedback"},{"location":"guides/refining-solutions/feedback/#introduction","text":"Data Preparer systematically explores a space of integration and cleaning options but can certainly make decisions that are not the ones the user would prefer. As a result, Data Preparer allows the user to refine how the data wrangle should happen, either by providing feedback on the results of the data preparation task or by changing parameters that control how Data Preparer behaves, or annotate the constructs that can contribute to a solution.","title":"Introduction"},{"location":"guides/refining-solutions/feedback/#how-to-use-feedback","text":"By clicking on end product after wrangling the data, you will be able to submit feedback. At the bottom of the page, you can select either cells (individual attribute values) or tuples (complete rows), then by clicking on the entries in the table, you will mark the cells/rows with the selected attribute (green - relevant, red - non-relevant). Clicking Submit feedback saves your choices. If you are not happy with a change, scroll to the feedback section and remove it.","title":"How to use feedback"},{"location":"guides/refining-solutions/intro/","text":"Refining Solutions Motivation Data Preparer is designed to automate data preparation, which means searching the space of opportunities for cleaning and combining data sets, taking into account user-specified priorities. Data Preparer systematically explores a space of integration and cleaning options, guided by evidence, but can certainly make decisions that are not the ones the user would prefer. This could be because certain data sets are out-of-date or unreliable in ways that only an expert could discern because Data Preparer misses specific opportunities, or because some of the decisions are inappropriate. Refine Solutions features Data Preparer allows the user to refine how Data Preparer wrangles the data. by providing feedback on the results of the data preparation task by changing parameters that control how Data Preparer behaves by annotating the constructs that can contribute to a solution","title":"Motivation"},{"location":"guides/refining-solutions/intro/#refining-solutions","text":"","title":"Refining Solutions"},{"location":"guides/refining-solutions/intro/#motivation","text":"Data Preparer is designed to automate data preparation, which means searching the space of opportunities for cleaning and combining data sets, taking into account user-specified priorities. Data Preparer systematically explores a space of integration and cleaning options, guided by evidence, but can certainly make decisions that are not the ones the user would prefer. This could be because certain data sets are out-of-date or unreliable in ways that only an expert could discern because Data Preparer misses specific opportunities, or because some of the decisions are inappropriate.","title":"Motivation"},{"location":"guides/refining-solutions/intro/#refine-solutions-features","text":"Data Preparer allows the user to refine how Data Preparer wrangles the data. by providing feedback on the results of the data preparation task by changing parameters that control how Data Preparer behaves by annotating the constructs that can contribute to a solution","title":"Refine Solutions features"},{"location":"guides/refining-solutions/steering/","text":"Introduction to steering Steering referes to the process of altering parameters throughout the pipeline to change the final result. The processing pipeline applied by Data Preparer can be seen at the top of the control panel tab, as illustrated below. The control panel allows individual steps to be toggled on and off (all are on, as indicated by the tick symbol), though note that there are dependencies, so deselecting some steps will lead to others being deselected (which is visualised by removing their ticks). Selecting the play symbol(\u25b7) runs the data preparation process up to that stage, and can allow rapid scrutiny of the results or part of the complete pipeline. Individual parameter control Matching match strategy : schema \u2013 use only attribute names for matching. instance \u2013 use only instance evidence when there are instances in the data context, and when not use schema. schema + instances \u2013 combine the evidence from schema and instance matching into a single score. We recommend schema + instances, as it takes into account all available evidence. match threshold : Value in the range 0 to 1. Discards all matches with a score below the threshold. We recommend experimenting with this threshold, though a value of around 0.4 is often appropriate, avoiding lots of spurious matches while including most correct ones. match instances max : Maximum number of values used in instance matching. 0 means that all the available data is used. Note that with large collections, a high value here will slow down the matching process. It is possible to exclude a match from data wrangling by clicking on matches text. You can select the area for a match in the exclude column to toggle the exclusion of the match from further processing. Excluding a match means that the associated source attribute will not be considered as a source for data for populating the matched target attribute. Using the same menu, you can insert a match that the system has missed using add match from the submenu. Selecting add match causes the new match to be created. Repair Data repair will generate functional dependencies strictly, and then use them for repair unless they are switched off. Transform minimum percent : Minimum percentage of the source values that should participate in the training data for the rule inference. Note: Format transformation infers rules that reformat from source values into the format of corresponding target values. minimum size : Related to minimum percent, but specified as a number of values instead of a percentage. fail limit : Controls how many attempts the synthesis algorithm should have before concluding that a transformation cannot be found for an attribute. The fail limit should be less than the minimum size. source limit : Number of values from the source that will be considered for use in examples (or 0 for unbounded). This should not be less than the minimum size. In general, attributes that contain more different formats will require higher values for all these parameters, but higher values lead to longer runtimes. We recommend that none of minimum percent, minimum size or source limit should be below 10, and that fail limit should be 5 or more. Accessing the transform details by clicking on the card, provides the option to verify the transformations that have been carried out. It is also possible to define custom transformation examples that will instruct the process how to transform source attribute value. These user-defined examples will be used in conjunction to the ones obtained from the data context. Rather than revealing the internal representation of a rule, here the user can accept or reject a rule based on its effect. The user can also add custom transformation expressions to modify the contents of source attributes directly by selecting add expression . Expressions can either modify an existing attribute, or add a new one with the results of the expression as its contents. For instance, the expression: UPPER(\u201dsource_table\u201d.\u201dattribute\u201d) will convert the values of the defined attribute to upper case. Expressions can contain any number of functions and attributes. In practice, running an expression will result in an execution of a SQL query of the form: UPDATE source_table SET attribute=expression Transformations are not part of the wrangling process. Transformation expressions, as well as attribute reformats, are both executed: * at the request of the user * upon scenario initialisation, if the property transform.apply_transformations has been set to yes. Profile For candidate key descovery: Detect candidate keys : Whether to search for candidate key attributes or not. This is enabled by default. Max candidate key size : the maximum number of attributes that can be used in a candidate key. We recommend this is kept quite low (e.g. 1), as the number of candidate keys in tables with many attributes can grow rapidly. For inclusion dependency discovery: Overlap threshold : Only inclusion dependencies with at least this fraction of overlap will be used in joins. Note that inclusion dependencies are kept when the overlap in either direction exceeds the overlap threshold. Matches only : Restrict inclusion dependency discovery only to source tables that have at least one attribute matching an end product attribute. Candidate keys only : Restrict inclusion dependency discovery only to attributes that are candidate keys. These latter two parameters can be used to reduce the number of inclusion dependencies retained, to those that are most relevant to the wrangling task at hand. It is possible to exclude an inclusion dependency or a candidate key from data wrangling by clicking on the inclusion dependencies and candidate keys in the explain window. As with matches above, the exclude column can be used to toggle the availability of the inclusion dependency or candidate key in the wrangling process. Note: Inclusion dependencies are used in mapping generation to identify when joins can be carried out, so excluding an inclusion dependency means that the attributes in the dependency will not be joined. Similarly, candidate keys are used in mapping generation to determine which attributes are used to join tables, and which join operator to use. As a result, excluding a candidate key will reduce the likelihood that the attributes involved in the key will participate in join conditions. Mapping Generation mapping top k : Maximum number of mappings that will be produced by mapping generation. We suggest starting with a small number (e.g., 10), as evaluating individual mappings can be expensive, and there may be many candidate mappings that provide rather partial answers. If the generated mappings continue to be useful, the number can be increased incrementally. mapping size max : Maximum number of tables that can contribute to a mapping. This limits the number of joins that can appear in a mapping. Note that although data preparer unions the results of different mappings, mappings themselves do not contain the results of union operations. We suggest starting with a small number (e.g., 5), and checking that the generated mappings make sense. It is possible to exclude a mapping from data wrangling by clicking on the mappings in the explain window. As with matches above, the exclude column can be used to toggle the exclusion of the mapping from the wrangling process. Note that excluding a mapping also excludes mappings that are subsumed by that mapping (i.e. simpler mappings that can be obtained by removing joins from this mapping). As a result, excluding a mapping is quite a blunt instrument; finer grained control over mapping generation can be obtained by editing profiling data. Tuple Selection criteria limit : the number of rows used when evaluating a criterion on a source. Set to 0 for the whole source to be used. product size : the number of rows to include in the end product. Control the Role of Sources However, as well as steering the steps in the process, Data Preparer also allows the roles of the sources to be individually configured, allowing fine grained control over how they contribute to the data preparation process For each attribute in a scenario, it is possible to toggle whether or not that attribute should be considered by each of the steps in the data wrangling process. For each of these steps, excluding an attribute means: match : do not match this attribute with the target, and thus do not use this attribute as a source of values for the target. repair : do not fill in empty values for this attribute using repair rules. transform : do not reformat the values of this attribute. candidate key : do not consider this attribute for inclusion in a candidate key. inclusion dependency : do not compute inclusion dependencies for this attribute. join : do not use this attribute in join conditions. The action icon to the left of the attributes opens context menu with shortcuts to attribute-specific actions including: add to target schema add match add transformation examples reformat attributem add transformation expression Context usage Wrangling Stage Data Context Types Comment Matching Reference, Master, Example All evidence can be helpful Repair Reference, Master Repair needs dependable functional dependencies Transformation Reference, Master, Example Will need comprehensive / consistent data Profiling Does not use data context Mapping Generation Does not use data context Mapping Selection Reference, Master, Example Depending on the criterion","title":"Steering"},{"location":"guides/refining-solutions/steering/#introduction-to-steering","text":"Steering referes to the process of altering parameters throughout the pipeline to change the final result. The processing pipeline applied by Data Preparer can be seen at the top of the control panel tab, as illustrated below. The control panel allows individual steps to be toggled on and off (all are on, as indicated by the tick symbol), though note that there are dependencies, so deselecting some steps will lead to others being deselected (which is visualised by removing their ticks). Selecting the play symbol(\u25b7) runs the data preparation process up to that stage, and can allow rapid scrutiny of the results or part of the complete pipeline.","title":"Introduction to steering"},{"location":"guides/refining-solutions/steering/#individual-parameter-control","text":"","title":"Individual parameter control"},{"location":"guides/refining-solutions/steering/#matching","text":"match strategy : schema \u2013 use only attribute names for matching. instance \u2013 use only instance evidence when there are instances in the data context, and when not use schema. schema + instances \u2013 combine the evidence from schema and instance matching into a single score. We recommend schema + instances, as it takes into account all available evidence. match threshold : Value in the range 0 to 1. Discards all matches with a score below the threshold. We recommend experimenting with this threshold, though a value of around 0.4 is often appropriate, avoiding lots of spurious matches while including most correct ones. match instances max : Maximum number of values used in instance matching. 0 means that all the available data is used. Note that with large collections, a high value here will slow down the matching process. It is possible to exclude a match from data wrangling by clicking on matches text. You can select the area for a match in the exclude column to toggle the exclusion of the match from further processing. Excluding a match means that the associated source attribute will not be considered as a source for data for populating the matched target attribute. Using the same menu, you can insert a match that the system has missed using add match from the submenu. Selecting add match causes the new match to be created.","title":"Matching"},{"location":"guides/refining-solutions/steering/#repair","text":"Data repair will generate functional dependencies strictly, and then use them for repair unless they are switched off.","title":"Repair"},{"location":"guides/refining-solutions/steering/#transform","text":"minimum percent : Minimum percentage of the source values that should participate in the training data for the rule inference. Note: Format transformation infers rules that reformat from source values into the format of corresponding target values. minimum size : Related to minimum percent, but specified as a number of values instead of a percentage. fail limit : Controls how many attempts the synthesis algorithm should have before concluding that a transformation cannot be found for an attribute. The fail limit should be less than the minimum size. source limit : Number of values from the source that will be considered for use in examples (or 0 for unbounded). This should not be less than the minimum size. In general, attributes that contain more different formats will require higher values for all these parameters, but higher values lead to longer runtimes. We recommend that none of minimum percent, minimum size or source limit should be below 10, and that fail limit should be 5 or more. Accessing the transform details by clicking on the card, provides the option to verify the transformations that have been carried out. It is also possible to define custom transformation examples that will instruct the process how to transform source attribute value. These user-defined examples will be used in conjunction to the ones obtained from the data context. Rather than revealing the internal representation of a rule, here the user can accept or reject a rule based on its effect. The user can also add custom transformation expressions to modify the contents of source attributes directly by selecting add expression . Expressions can either modify an existing attribute, or add a new one with the results of the expression as its contents. For instance, the expression: UPPER(\u201dsource_table\u201d.\u201dattribute\u201d) will convert the values of the defined attribute to upper case. Expressions can contain any number of functions and attributes. In practice, running an expression will result in an execution of a SQL query of the form: UPDATE source_table SET attribute=expression Transformations are not part of the wrangling process. Transformation expressions, as well as attribute reformats, are both executed: * at the request of the user * upon scenario initialisation, if the property transform.apply_transformations has been set to yes.","title":"Transform"},{"location":"guides/refining-solutions/steering/#profile","text":"","title":"Profile"},{"location":"guides/refining-solutions/steering/#for-candidate-key-descovery","text":"Detect candidate keys : Whether to search for candidate key attributes or not. This is enabled by default. Max candidate key size : the maximum number of attributes that can be used in a candidate key. We recommend this is kept quite low (e.g. 1), as the number of candidate keys in tables with many attributes can grow rapidly.","title":"For candidate key descovery:"},{"location":"guides/refining-solutions/steering/#for-inclusion-dependency-discovery","text":"Overlap threshold : Only inclusion dependencies with at least this fraction of overlap will be used in joins. Note that inclusion dependencies are kept when the overlap in either direction exceeds the overlap threshold. Matches only : Restrict inclusion dependency discovery only to source tables that have at least one attribute matching an end product attribute. Candidate keys only : Restrict inclusion dependency discovery only to attributes that are candidate keys. These latter two parameters can be used to reduce the number of inclusion dependencies retained, to those that are most relevant to the wrangling task at hand. It is possible to exclude an inclusion dependency or a candidate key from data wrangling by clicking on the inclusion dependencies and candidate keys in the explain window. As with matches above, the exclude column can be used to toggle the availability of the inclusion dependency or candidate key in the wrangling process. Note: Inclusion dependencies are used in mapping generation to identify when joins can be carried out, so excluding an inclusion dependency means that the attributes in the dependency will not be joined. Similarly, candidate keys are used in mapping generation to determine which attributes are used to join tables, and which join operator to use. As a result, excluding a candidate key will reduce the likelihood that the attributes involved in the key will participate in join conditions.","title":"For inclusion dependency discovery:"},{"location":"guides/refining-solutions/steering/#mapping-generation","text":"mapping top k : Maximum number of mappings that will be produced by mapping generation. We suggest starting with a small number (e.g., 10), as evaluating individual mappings can be expensive, and there may be many candidate mappings that provide rather partial answers. If the generated mappings continue to be useful, the number can be increased incrementally. mapping size max : Maximum number of tables that can contribute to a mapping. This limits the number of joins that can appear in a mapping. Note that although data preparer unions the results of different mappings, mappings themselves do not contain the results of union operations. We suggest starting with a small number (e.g., 5), and checking that the generated mappings make sense. It is possible to exclude a mapping from data wrangling by clicking on the mappings in the explain window. As with matches above, the exclude column can be used to toggle the exclusion of the mapping from the wrangling process. Note that excluding a mapping also excludes mappings that are subsumed by that mapping (i.e. simpler mappings that can be obtained by removing joins from this mapping). As a result, excluding a mapping is quite a blunt instrument; finer grained control over mapping generation can be obtained by editing profiling data.","title":"Mapping Generation"},{"location":"guides/refining-solutions/steering/#tuple-selection","text":"criteria limit : the number of rows used when evaluating a criterion on a source. Set to 0 for the whole source to be used. product size : the number of rows to include in the end product.","title":"Tuple Selection"},{"location":"guides/refining-solutions/steering/#control-the-role-of-sources","text":"However, as well as steering the steps in the process, Data Preparer also allows the roles of the sources to be individually configured, allowing fine grained control over how they contribute to the data preparation process For each attribute in a scenario, it is possible to toggle whether or not that attribute should be considered by each of the steps in the data wrangling process. For each of these steps, excluding an attribute means: match : do not match this attribute with the target, and thus do not use this attribute as a source of values for the target. repair : do not fill in empty values for this attribute using repair rules. transform : do not reformat the values of this attribute. candidate key : do not consider this attribute for inclusion in a candidate key. inclusion dependency : do not compute inclusion dependencies for this attribute. join : do not use this attribute in join conditions. The action icon to the left of the attributes opens context menu with shortcuts to attribute-specific actions including: add to target schema add match add transformation examples reformat attributem add transformation expression","title":"Control the Role of Sources"},{"location":"guides/refining-solutions/steering/#context-usage","text":"Wrangling Stage Data Context Types Comment Matching Reference, Master, Example All evidence can be helpful Repair Reference, Master Repair needs dependable functional dependencies Transformation Reference, Master, Example Will need comprehensive / consistent data Profiling Does not use data context Mapping Generation Does not use data context Mapping Selection Reference, Master, Example Depending on the criterion","title":"Context usage"},{"location":"guides/setting-priorities/criterias/","text":"Prioritizing Results using Criteria How it works The User Context contains a collection of criteria, each identifying a feature of the result that is important to the user. Each criterion has a weight set by the user. This is used to indicate the importance of criteria compared to each other. These criteria are used by Data Preparer to assign scores to mappings, where each mapping represents a way of populating the target from one or more sources The target is then populated with data from the highest-scoring mappings. How to add a criterion Head over to the User Context tab and click on Add Criterion . A modal to add a criterion will appear. Select the criteria, element and weight. Inverting a criterion Inverting a criterion means that when selecting tuples, those that do not satisfy it will be preferred over those that do. Note: This essentially negates your criterion Updating weights Enter a new value in the weight inputs and click on update weights to change weights. Supported criteria No parameters required completeness wrt. empty values: the ratio of the values that are null or empty over the total number of values completeness wrt. tuples: the ratio of the tuples with non-empty values over the total number of tuples. It can only be evaluated on a table type numeric: the ratio of the values that represent numbers over the total number of values type datetime: the ratio of the values that represent dates or times over the total number of values is email: the ratio of the values that represent emails over the total number of values is URI: the ratio of the values that represent URIs over the total number of values contain numbers: the ratio of the values that contain a number over the total number of values contain letters: the ratio of the values that contain a letter over the total number of values contain punctuations: the ratio of the values that contain a punctuation character over the total number of values. Note: punctuation is any character that is not a letter or a number or whitespace contain email: the ratio of the values that contain an email address over the total number of values contain URI: the ratio of the values that contain a URI over the total number of values Data context required completeness wrt. data context: the ratio of the values included in the Master or Example data over the total number of values consistency wrt. data context: the ratio of the values included in the Reference or Example data over the total number of values Feedback required relevance wrt. feedback: the ratio of the values marked as relevant over the total number of values","title":"Using Criteria"},{"location":"guides/setting-priorities/criterias/#prioritizing-results-using-criteria","text":"","title":"Prioritizing Results using Criteria"},{"location":"guides/setting-priorities/criterias/#how-it-works","text":"The User Context contains a collection of criteria, each identifying a feature of the result that is important to the user. Each criterion has a weight set by the user. This is used to indicate the importance of criteria compared to each other. These criteria are used by Data Preparer to assign scores to mappings, where each mapping represents a way of populating the target from one or more sources The target is then populated with data from the highest-scoring mappings.","title":"How it works"},{"location":"guides/setting-priorities/criterias/#how-to-add-a-criterion","text":"Head over to the User Context tab and click on Add Criterion . A modal to add a criterion will appear. Select the criteria, element and weight.","title":"How to add a criterion"},{"location":"guides/setting-priorities/criterias/#inverting-a-criterion","text":"Inverting a criterion means that when selecting tuples, those that do not satisfy it will be preferred over those that do. Note: This essentially negates your criterion","title":"Inverting a criterion"},{"location":"guides/setting-priorities/criterias/#updating-weights","text":"Enter a new value in the weight inputs and click on update weights to change weights.","title":"Updating weights"},{"location":"guides/setting-priorities/criterias/#supported-criteria","text":"","title":"Supported criteria"},{"location":"guides/setting-priorities/criterias/#no-parameters-required","text":"completeness wrt. empty values: the ratio of the values that are null or empty over the total number of values completeness wrt. tuples: the ratio of the tuples with non-empty values over the total number of tuples. It can only be evaluated on a table type numeric: the ratio of the values that represent numbers over the total number of values type datetime: the ratio of the values that represent dates or times over the total number of values is email: the ratio of the values that represent emails over the total number of values is URI: the ratio of the values that represent URIs over the total number of values contain numbers: the ratio of the values that contain a number over the total number of values contain letters: the ratio of the values that contain a letter over the total number of values contain punctuations: the ratio of the values that contain a punctuation character over the total number of values. Note: punctuation is any character that is not a letter or a number or whitespace contain email: the ratio of the values that contain an email address over the total number of values contain URI: the ratio of the values that contain a URI over the total number of values","title":"No parameters required"},{"location":"guides/setting-priorities/criterias/#data-context-required","text":"completeness wrt. data context: the ratio of the values included in the Master or Example data over the total number of values consistency wrt. data context: the ratio of the values included in the Reference or Example data over the total number of values","title":"Data context required"},{"location":"guides/setting-priorities/criterias/#feedback-required","text":"relevance wrt. feedback: the ratio of the values marked as relevant over the total number of values","title":"Feedback required"},{"location":"guides/setting-priorities/filters/","text":"Selecting Results using Criteria How it works The User Context contains a collection of criteria, each identifying a feature of the result that is important to the user. Each criterion has a weight set by the user. This is used to indicate the importance of criteria compared to each other. These criteria are used by Data Preparer to assign scores to mappings, where each mapping represents a way of populating the target from one or more sources The target is then populated with data from the highest-scoring mappings. Inverting a filter Inverting a filter means that only tuples which do not satisfy the filter will be allowed to populate the end product. Note: This essentially negates your filter. How to add a filter Head over to the User Context tab and click on Add filter . A modal to add a filter will appear. Select your filter and element. Supported filters No parameters required completeness wrt. empty values: the ratio of the values that are null or empty over the total number of values completeness wrt. tuples: the ratio of the tuples with non-empty values over the total number of tuples. It can only be evaluated on a table type numeric: the ratio of the values that represent numbers over the total number of values type datetime: the ratio of the values that represent dates or times over the total number of values is email: the ratio of the values that represent emails over the total number of values is URI: the ratio of the values that represent URIs over the total number of values contain numbers: the ratio of the values that contain a number over the total number of values contain letters: the ratio of the values that contain a letter over the total number of values contain punctuations: the ratio of the values that contain a punctuation character over the total number of values. Note: punctuation is any character that is not a letter or a number or whitespace contain email: the ratio of the values that contain an email address over the total number of values contain URI: the ratio of the values that contain a URI over the total number of values Data context required completeness wrt. data context: the ratio of the values included in the Master or Example data over the total number of values consistency wrt. data context: the ratio of the values included in the Reference or Example data over the total number of values Feedback required relevance wrt. feedback: the ratio of the values marked as relevant over the total number of values","title":"Using Filters"},{"location":"guides/setting-priorities/filters/#selecting-results-using-criteria","text":"","title":"Selecting Results using Criteria"},{"location":"guides/setting-priorities/filters/#how-it-works","text":"The User Context contains a collection of criteria, each identifying a feature of the result that is important to the user. Each criterion has a weight set by the user. This is used to indicate the importance of criteria compared to each other. These criteria are used by Data Preparer to assign scores to mappings, where each mapping represents a way of populating the target from one or more sources The target is then populated with data from the highest-scoring mappings.","title":"How it works"},{"location":"guides/setting-priorities/filters/#inverting-a-filter","text":"Inverting a filter means that only tuples which do not satisfy the filter will be allowed to populate the end product. Note: This essentially negates your filter.","title":"Inverting a filter"},{"location":"guides/setting-priorities/filters/#how-to-add-a-filter","text":"Head over to the User Context tab and click on Add filter . A modal to add a filter will appear. Select your filter and element.","title":"How to add a filter"},{"location":"guides/setting-priorities/filters/#supported-filters","text":"","title":"Supported filters"},{"location":"guides/setting-priorities/filters/#no-parameters-required","text":"completeness wrt. empty values: the ratio of the values that are null or empty over the total number of values completeness wrt. tuples: the ratio of the tuples with non-empty values over the total number of tuples. It can only be evaluated on a table type numeric: the ratio of the values that represent numbers over the total number of values type datetime: the ratio of the values that represent dates or times over the total number of values is email: the ratio of the values that represent emails over the total number of values is URI: the ratio of the values that represent URIs over the total number of values contain numbers: the ratio of the values that contain a number over the total number of values contain letters: the ratio of the values that contain a letter over the total number of values contain punctuations: the ratio of the values that contain a punctuation character over the total number of values. Note: punctuation is any character that is not a letter or a number or whitespace contain email: the ratio of the values that contain an email address over the total number of values contain URI: the ratio of the values that contain a URI over the total number of values","title":"No parameters required"},{"location":"guides/setting-priorities/filters/#data-context-required","text":"completeness wrt. data context: the ratio of the values included in the Master or Example data over the total number of values consistency wrt. data context: the ratio of the values included in the Reference or Example data over the total number of values","title":"Data context required"},{"location":"guides/setting-priorities/filters/#feedback-required","text":"relevance wrt. feedback: the ratio of the values marked as relevant over the total number of values","title":"Feedback required"},{"location":"guides/setting-priorities/intro/","text":"Setting Priorities Motivation By default, Data Preparer takes responsibility for exploring the space of data preparation options. These options may represent different data sources; some may provide better quality or more relevant data and different ways of wrangling this data. This may be more or less suitable for the task at hand. Often there may be trade-offs. Example: Obtaining a more significant result from the wrangling process may involve certain compromises to quality. Data scientists also face these trade-offs in manual data preparation. User context feature In Data Preparer, a user can select data that meet their specified priorities by configuring their data priorities through the User Context tab. There are 2 methods to set the priorities using Criteria and Filsetters , which will be discussed in the next section. Criteria and Filters Criteria are set to compare tuples and choose the better tuples by their weight Filters are set to exclude values that are not suitable for the analysis Note: Filters are applied after criteria are used, thus leading to smaller results than requested as the product size.","title":"Motivation"},{"location":"guides/setting-priorities/intro/#setting-priorities","text":"","title":"Setting Priorities"},{"location":"guides/setting-priorities/intro/#motivation","text":"By default, Data Preparer takes responsibility for exploring the space of data preparation options. These options may represent different data sources; some may provide better quality or more relevant data and different ways of wrangling this data. This may be more or less suitable for the task at hand. Often there may be trade-offs. Example: Obtaining a more significant result from the wrangling process may involve certain compromises to quality. Data scientists also face these trade-offs in manual data preparation.","title":"Motivation"},{"location":"guides/setting-priorities/intro/#user-context-feature","text":"In Data Preparer, a user can select data that meet their specified priorities by configuring their data priorities through the User Context tab. There are 2 methods to set the priorities using Criteria and Filsetters , which will be discussed in the next section.","title":"User context feature"},{"location":"guides/setting-priorities/intro/#criteria-and-filters","text":"Criteria are set to compare tuples and choose the better tuples by their weight Filters are set to exclude values that are not suitable for the analysis Note: Filters are applied after criteria are used, thus leading to smaller results than requested as the product size.","title":"Criteria and Filters"},{"location":"open-source/build-instructions/","text":"","title":"Build instructions"},{"location":"pipeline/map/","text":"Map transducer How it works This transducer takes in all the data sources, relationships and similarity scores generated by profiling. Datapreparer then joins the data together to create combined data sources. To access this, click on the mapping generation node in the latest wrangle result section : An example If we run the provided ManualBook1.scenario file and perform the wrangle, you will get the following output: So what does this mean? This output tells us that datapreparer has found three candidate mappings between data sources. These are: Riverside and RiversideBooks on the title column Riverside and TitleAuthor on the title column GoodRead and TitleAuthor on the title column Looking carefully, we can see that Datapreparer says we can perform an outer join on these columns. The join is done automatically for us. Viewing a mapping If you look at the output in the above example, you will see the mapping names highlighted. Clicking on this gives us some key details of the mapping. Here is the result of clicking m02 : On this page, we can see: provenance - the data sources used in the mapping a diagram that shows the mapping as an input/output diagram. Clicking on each element inside the diagram gives you the ability to view user context, data sources, attributes and properties. a table view of the joined data If you want to explore the original data, you can click on a data source. This will take you to a page to view the data source. Look at the viewing guide .","title":"Map"},{"location":"pipeline/map/#map-transducer","text":"","title":"Map transducer"},{"location":"pipeline/map/#how-it-works","text":"This transducer takes in all the data sources, relationships and similarity scores generated by profiling. Datapreparer then joins the data together to create combined data sources. To access this, click on the mapping generation node in the latest wrangle result section :","title":"How it works"},{"location":"pipeline/map/#an-example","text":"If we run the provided ManualBook1.scenario file and perform the wrangle, you will get the following output: So what does this mean? This output tells us that datapreparer has found three candidate mappings between data sources. These are: Riverside and RiversideBooks on the title column Riverside and TitleAuthor on the title column GoodRead and TitleAuthor on the title column Looking carefully, we can see that Datapreparer says we can perform an outer join on these columns. The join is done automatically for us.","title":"An example"},{"location":"pipeline/map/#viewing-a-mapping","text":"If you look at the output in the above example, you will see the mapping names highlighted. Clicking on this gives us some key details of the mapping. Here is the result of clicking m02 : On this page, we can see: provenance - the data sources used in the mapping a diagram that shows the mapping as an input/output diagram. Clicking on each element inside the diagram gives you the ability to view user context, data sources, attributes and properties. a table view of the joined data If you want to explore the original data, you can click on a data source. This will take you to a page to view the data source. Look at the viewing guide .","title":"Viewing a mapping"},{"location":"pipeline/overview/","text":"","title":"Overview"},{"location":"pipeline/profile/","text":"Profile Component How it works Candidate Keys: A candidate key is a collection of attributes, the values of which uniquely identify a row (i.e., there are not two rows with identical values for those attributes). In Data Preparer, candidate key information informs what type of join operation is used to combine tables. Inclusion Dependencies: Inclusion dependencies describe overlaps between the values in the attributes of columns in sources. In Data Preparer, inclusion dependencies are used to identify where there is an opportunity to join two tables. Example In the figure below, it is indicated that the values inGoodread.details are contained within those of RainforestBooks.title, with an overlap of 0.667. This indicates that 66.7% of the values in Goodread.details can be found in RainforestBooks.title. Note that this notion of overlap is not symmetrical, and we can also see that the reverse overlap (i.e. the fraction of values in RainforestBooks.title that are also contained in Goodread.details is only 0.278). Configurations in the control panel Profiling parameters configure the discovery of candidate keys and the discovery of inclusion dependencies. Candidate key discovery parameters Detect candidate keys : whether to search for candidate key attributes or not. This is enabled by default. Max candidate key size : the maximum number of attributes that can be used in a candidate key. We recommend this is kept quite low (e.g. 1), as the number of candidate keys in tables with many attributes can grow rapidly Inclusion dependency discovery parameters Overlap threshold : only inclusion dependencies with at least this fraction of overlap will be used in joins. Note that inclusion dependencies are kept when the overlap in either direction exceeds the overlap threshold. Matches only : Restrict inclusion dependency discovery only to source tables that have at least one attribute matching an end product attribute. Candidate keys only : Restrict inclusion dependency discovery only to attributes that are candidate keys. Note: These latter two parameters can be used to reduce the number of inclusion dependencies retained, to those that are most relevant to the wrangling task at hand. Excluding an inclusion dependency It is possible to exclude an inclusion dependency or a candidate key in the explain window for the profiling component. As with the matches component, the exclude column can be used to toggle the availability of the inclusion dependency or candidate key in the wrangling process. In Data Preparer, inclusion dependencies are used in mapping generation to identify when joins can be carried out, so excluding an inclusion dependency means that the attributes in the dependency will not be joined. Similarly, candidate keys are used in mapping generation to determine which attributes are used to join tables, and which join operator to use. As a result, excluding a candidate key will reduce the likelihood that the attributes involved in the key will participate in join conditions.","title":"Profile"},{"location":"pipeline/profile/#profile-component","text":"","title":"Profile Component"},{"location":"pipeline/profile/#how-it-works","text":"Candidate Keys: A candidate key is a collection of attributes, the values of which uniquely identify a row (i.e., there are not two rows with identical values for those attributes). In Data Preparer, candidate key information informs what type of join operation is used to combine tables. Inclusion Dependencies: Inclusion dependencies describe overlaps between the values in the attributes of columns in sources. In Data Preparer, inclusion dependencies are used to identify where there is an opportunity to join two tables.","title":"How it works"},{"location":"pipeline/profile/#example","text":"In the figure below, it is indicated that the values inGoodread.details are contained within those of RainforestBooks.title, with an overlap of 0.667. This indicates that 66.7% of the values in Goodread.details can be found in RainforestBooks.title. Note that this notion of overlap is not symmetrical, and we can also see that the reverse overlap (i.e. the fraction of values in RainforestBooks.title that are also contained in Goodread.details is only 0.278).","title":"Example"},{"location":"pipeline/profile/#configurations-in-the-control-panel","text":"Profiling parameters configure the discovery of candidate keys and the discovery of inclusion dependencies.","title":"Configurations in the control panel"},{"location":"pipeline/profile/#candidate-key-discovery-parameters","text":"Detect candidate keys : whether to search for candidate key attributes or not. This is enabled by default. Max candidate key size : the maximum number of attributes that can be used in a candidate key. We recommend this is kept quite low (e.g. 1), as the number of candidate keys in tables with many attributes can grow rapidly","title":"Candidate key discovery parameters"},{"location":"pipeline/profile/#inclusion-dependency-discovery-parameters","text":"Overlap threshold : only inclusion dependencies with at least this fraction of overlap will be used in joins. Note that inclusion dependencies are kept when the overlap in either direction exceeds the overlap threshold. Matches only : Restrict inclusion dependency discovery only to source tables that have at least one attribute matching an end product attribute. Candidate keys only : Restrict inclusion dependency discovery only to attributes that are candidate keys. Note: These latter two parameters can be used to reduce the number of inclusion dependencies retained, to those that are most relevant to the wrangling task at hand.","title":"Inclusion dependency discovery parameters"},{"location":"pipeline/profile/#excluding-an-inclusion-dependency","text":"It is possible to exclude an inclusion dependency or a candidate key in the explain window for the profiling component. As with the matches component, the exclude column can be used to toggle the availability of the inclusion dependency or candidate key in the wrangling process. In Data Preparer, inclusion dependencies are used in mapping generation to identify when joins can be carried out, so excluding an inclusion dependency means that the attributes in the dependency will not be joined. Similarly, candidate keys are used in mapping generation to determine which attributes are used to join tables, and which join operator to use. As a result, excluding a candidate key will reduce the likelihood that the attributes involved in the key will participate in join conditions.","title":"Excluding an inclusion dependency"},{"location":"pipeline/select/","text":"Select transducer How it works Tuple selection is the final step of the Datapreparer pipeline. It takes in the generated candidate mappings and applies all the joins to the data. Datapreparer combines the mappings to produce the output of the wrangle, what we call the end product . The output file can be downloaded as a CSV, Excel, JSON or avron file. You can then upload this file to a database or storage medium. An example We will again consider the ManualBooks1 scenario. If you go to last wrangle results and click tuple selection , you will see the following page: We get the following columns as a result: mapping - gives the id and link to the mapping applied provenance - what mapping occurs and the data sources involved completeness - the percentage of values that are blank in the resulting tuples size - number of tuples in the mapping selected tuples - number of tuples selected from the mapping Accessing the output The final step is to view the output of the wrangle. This is accessible from the end product tab. An example output is something like this, You can then click the download icon next to the table name and select the file type you want to download. You now have access to the wrangle output!","title":"Select"},{"location":"pipeline/select/#select-transducer","text":"","title":"Select transducer"},{"location":"pipeline/select/#how-it-works","text":"Tuple selection is the final step of the Datapreparer pipeline. It takes in the generated candidate mappings and applies all the joins to the data. Datapreparer combines the mappings to produce the output of the wrangle, what we call the end product . The output file can be downloaded as a CSV, Excel, JSON or avron file. You can then upload this file to a database or storage medium.","title":"How it works"},{"location":"pipeline/select/#an-example","text":"We will again consider the ManualBooks1 scenario. If you go to last wrangle results and click tuple selection , you will see the following page: We get the following columns as a result: mapping - gives the id and link to the mapping applied provenance - what mapping occurs and the data sources involved completeness - the percentage of values that are blank in the resulting tuples size - number of tuples in the mapping selected tuples - number of tuples selected from the mapping","title":"An example"},{"location":"pipeline/select/#accessing-the-output","text":"The final step is to view the output of the wrangle. This is accessible from the end product tab. An example output is something like this, You can then click the download icon next to the table name and select the file type you want to download. You now have access to the wrangle output!","title":"Accessing the output"},{"location":"pipeline/transform/","text":"Transform Component What it does A format transformation is a change to the way that attribute values are presented. For example, names in different sources could be of the form Forename Surname or Firstname Surname URLs can be written with or without the protocol (http, https). How it works In Data Preparer, the data context is assumed to provide the preferred representation for a type of data, and attributes that are used to populate a target attribute for which there are values in the data context may have their formats transformed to reflect that in the data context. Note: As the format transformation is inferred from examples, in practice format transformation only takes place when there are plenty examples in the data set and in the data context, and when the attributes in question have some form of regular structure (as in the date and URL examples above). Example In the explain page in the figure below, format transformations are described in terms of examples used for training, rather than the more difficult to interpret inferred rules. We can see that in the book promotions example, values in Riverside.isbn that have an ISBN prefix have been reformatted to have this removed, reflecting the representation in MasterData.isbn, and indeed that this transformation has been applied 20 times. Configurations in the control panel Format transformation infers rules that reformat from source values into the format of corresponding target values. Format transformation parameters minimum percent : This parameter indicates the minimum percentage of the source values that should participate in the training data for the rule inference. minimum size : This parameter is related to minimum percent, but is specified as a number of values instead of a percentage. fail limit : This parameter controls how many attempts the synthesis algorithm should have before concluding that a transformation cannot be found for an attribute. The fail limit should be less than the minimum size. source limit : This parameter is the number of values from the source that will be considered for use in examples (or 0 for unbounded). This should be more than the minimum size. Note: In general, attributes that contain more different formats will require higher values for all these parameters, but higher values lead to longer runtimes. We recommend that none of minimum percent, minimum size or source limit should be below 10, and that fail limit should be 5 or more. Transformations usages Add examples It is also possible to define custom transformation examples that will instruct the process how to transform source attribute values. This is done by selecting add examples from the submenu. These user-defined examples will be used in conjunction to the ones obtained from the data context. Rather than revealing the internal representation of a rule, here the user can accept or reject a rule based on its effect. Selecting the eye icon in the verify column gives rise to the display in the figure below, where the specific changes that have been made to the values in the isbn column can be seen. The user can then approve or reject the transformation using the buttons above the table Reformat From the submenu it is also possible to reformat source attributes to make them more consistent with those in the target. This is done by explicitly associating multiple (1 or more) source attributes with multiple target attributes. As a result, a derived table can be populated, with the reformatted source contents, informed by training data from the data context. As an example, consider a source table that contains an address attribute, while the target schema contains attributes number, street, and town. In order to make this source more consistent with the target, address can be aligned to number, street, town. Note: The many-to-many reformatting in this case is informed using data from the data context, though in this case the training data should all come from the same table in the data context. The data context must contain data that can be aligned with every distinct pattern of values in the source. So for example, if here are address values of the form 15 Cardiff Road, Swansea and 26 Mauldeth Road South, Manchester, then there will need to be examples that cover street names containing two and three words in the data context, and at least one of these examples should contain similar literal values to those in the source. This is most likely to be achieved without creating custom training data when the data context contains reference or master data. When attribute reformatting is successful, a new table is created, in the same source as the original source table. Add expression From the submenu, the user can also add custom transformation expressions to modify the contents of source attributes directly by selecting add expression. Expressions can either modify an existing attribute, or add a new one with the results of the expression as its contents. For instance, the expression: UPPER(\u201dsource_table\u201d.\u201dattribute\u201d) will convert the values of the defined attribute to upper case. Expressions can contain any number of functions and attributes. In practice, running an expression will result in an execution of a SQL query of the form: UPDATE source_table SET attribute=expression. Transformations are not part of the wrangling process. When it runs Transformation expressions, as well as attribute reformats, are both executed: at the request of the user, and upon scenario initialisation, if the property transform.apply_transformations has been set to yes.","title":"Transform"},{"location":"pipeline/transform/#transform-component","text":"","title":"Transform Component"},{"location":"pipeline/transform/#what-it-does","text":"A format transformation is a change to the way that attribute values are presented.","title":"What it does"},{"location":"pipeline/transform/#for-example","text":"names in different sources could be of the form Forename Surname or Firstname Surname URLs can be written with or without the protocol (http, https).","title":"For example,"},{"location":"pipeline/transform/#how-it-works","text":"In Data Preparer, the data context is assumed to provide the preferred representation for a type of data, and attributes that are used to populate a target attribute for which there are values in the data context may have their formats transformed to reflect that in the data context. Note: As the format transformation is inferred from examples, in practice format transformation only takes place when there are plenty examples in the data set and in the data context, and when the attributes in question have some form of regular structure (as in the date and URL examples above).","title":"How it works"},{"location":"pipeline/transform/#example","text":"In the explain page in the figure below, format transformations are described in terms of examples used for training, rather than the more difficult to interpret inferred rules. We can see that in the book promotions example, values in Riverside.isbn that have an ISBN prefix have been reformatted to have this removed, reflecting the representation in MasterData.isbn, and indeed that this transformation has been applied 20 times.","title":"Example"},{"location":"pipeline/transform/#configurations-in-the-control-panel","text":"Format transformation infers rules that reformat from source values into the format of corresponding target values.","title":"Configurations in the control panel"},{"location":"pipeline/transform/#format-transformation-parameters","text":"minimum percent : This parameter indicates the minimum percentage of the source values that should participate in the training data for the rule inference. minimum size : This parameter is related to minimum percent, but is specified as a number of values instead of a percentage. fail limit : This parameter controls how many attempts the synthesis algorithm should have before concluding that a transformation cannot be found for an attribute. The fail limit should be less than the minimum size. source limit : This parameter is the number of values from the source that will be considered for use in examples (or 0 for unbounded). This should be more than the minimum size. Note: In general, attributes that contain more different formats will require higher values for all these parameters, but higher values lead to longer runtimes. We recommend that none of minimum percent, minimum size or source limit should be below 10, and that fail limit should be 5 or more.","title":"Format transformation parameters"},{"location":"pipeline/transform/#transformations-usages","text":"","title":"Transformations usages"},{"location":"pipeline/transform/#add-examples","text":"It is also possible to define custom transformation examples that will instruct the process how to transform source attribute values. This is done by selecting add examples from the submenu. These user-defined examples will be used in conjunction to the ones obtained from the data context. Rather than revealing the internal representation of a rule, here the user can accept or reject a rule based on its effect. Selecting the eye icon in the verify column gives rise to the display in the figure below, where the specific changes that have been made to the values in the isbn column can be seen. The user can then approve or reject the transformation using the buttons above the table","title":"Add examples"},{"location":"pipeline/transform/#reformat","text":"From the submenu it is also possible to reformat source attributes to make them more consistent with those in the target. This is done by explicitly associating multiple (1 or more) source attributes with multiple target attributes. As a result, a derived table can be populated, with the reformatted source contents, informed by training data from the data context. As an example, consider a source table that contains an address attribute, while the target schema contains attributes number, street, and town. In order to make this source more consistent with the target, address can be aligned to number, street, town. Note: The many-to-many reformatting in this case is informed using data from the data context, though in this case the training data should all come from the same table in the data context. The data context must contain data that can be aligned with every distinct pattern of values in the source. So for example, if here are address values of the form 15 Cardiff Road, Swansea and 26 Mauldeth Road South, Manchester, then there will need to be examples that cover street names containing two and three words in the data context, and at least one of these examples should contain similar literal values to those in the source. This is most likely to be achieved without creating custom training data when the data context contains reference or master data. When attribute reformatting is successful, a new table is created, in the same source as the original source table.","title":"Reformat"},{"location":"pipeline/transform/#add-expression","text":"From the submenu, the user can also add custom transformation expressions to modify the contents of source attributes directly by selecting add expression. Expressions can either modify an existing attribute, or add a new one with the results of the expression as its contents. For instance, the expression: UPPER(\u201dsource_table\u201d.\u201dattribute\u201d) will convert the values of the defined attribute to upper case. Expressions can contain any number of functions and attributes. In practice, running an expression will result in an execution of a SQL query of the form: UPDATE source_table SET attribute=expression. Transformations are not part of the wrangling process.","title":"Add expression"},{"location":"pipeline/transform/#when-it-runs","text":"Transformation expressions, as well as attribute reformats,","title":"When it runs"},{"location":"pipeline/transform/#are-both-executed","text":"at the request of the user, and upon scenario initialisation, if the property transform.apply_transformations has been set to yes.","title":"are both executed:"},{"location":"pipeline/transducers/match/","text":"Match transducer How it works It is the first transducer in the workflow. This transducer lists associations that have been identified between source and target attributes. The presence of a match indicates that Data Preparer considers the source attribute as a plausible source of values for the target attribute. Matches are used in Data Preparer to identify which source attributes can be used to populate which target attributes, and to identify tables that, when used together, may be able to populate more target attributes than they can alone. An example In the example, using ManualBook1.scenario , GoodRead.details has been matched with BookPromotions.title, with a similarity score of 0.532 as seen in the first row. These attributes have been matched because, although the attribute names are different, their values are similar. The score is in the range 0 to 1, where the higher the score the more similar they are deemed to be. Configurations in the control panel Format match parameters: 1. Match strategy : Here there are three options: schema, schema+instances and instances. Schema is where only the attribute names will be compared for similarity. Schema+Instances is where both attribute and instances will be compared for similarity Instances is where only attribute instances will be compared for similarity. 2. Match threshold : This is a number between 0 and 1 that defines the threshold below which all matches will be discarded. 3. Match instances max : This is the number of instances to compare when evaluating similarity between two attributes. Note: Set to 0 for unlimited.","title":"Match transducer"},{"location":"pipeline/transducers/match/#match-transducer","text":"","title":"Match transducer"},{"location":"pipeline/transducers/match/#how-it-works","text":"It is the first transducer in the workflow. This transducer lists associations that have been identified between source and target attributes. The presence of a match indicates that Data Preparer considers the source attribute as a plausible source of values for the target attribute. Matches are used in Data Preparer to identify which source attributes can be used to populate which target attributes, and to identify tables that, when used together, may be able to populate more target attributes than they can alone.","title":"How it works"},{"location":"pipeline/transducers/match/#an-example","text":"In the example, using ManualBook1.scenario , GoodRead.details has been matched with BookPromotions.title, with a similarity score of 0.532 as seen in the first row. These attributes have been matched because, although the attribute names are different, their values are similar. The score is in the range 0 to 1, where the higher the score the more similar they are deemed to be.","title":"An example"},{"location":"pipeline/transducers/match/#configurations-in-the-control-panel","text":"","title":"Configurations in the control panel"},{"location":"pipeline/transducers/match/#format-match-parameters","text":"1. Match strategy : Here there are three options: schema, schema+instances and instances. Schema is where only the attribute names will be compared for similarity. Schema+Instances is where both attribute and instances will be compared for similarity Instances is where only attribute instances will be compared for similarity. 2. Match threshold : This is a number between 0 and 1 that defines the threshold below which all matches will be discarded. 3. Match instances max : This is the number of instances to compare when evaluating similarity between two attributes. Note: Set to 0 for unlimited.","title":"Format match parameters:"},{"location":"pipeline/transducers/repair/","text":"Repair transducer How it works Repair Rules: A repair rule uses data context data to fill in gaps in source data. Repair rules are based on functional dependencies, of the form ( determinant \u2192 dependent ). A functional dependency indicates that a value for the determinant uniquely identifies a value for the dependent . An example In Data Preparer, functional dependencies are inferred from the data context as shown in the image. In practice, whenever the attributes in a functional dependency found in the data context can be aligned with attributes in a data set, missing values for the dependent attribute in the data set can be replaced with the corresponding value from the data context. For example, the dependency ( MasterData.title \u2192 MasterData.rating ) can be used to populate missing values for rating in Riverside books, and we can see that 16 values for Riverside.rating have been updated using this dependency. Configurations in the control panel Format match parameters: Apply rules : Leave the box: - unchecked in order to verify whether to apply or discard suggested repairs. - checked to repair sources directly, based on the discovered rules, in which case you can still view applied repairs.git add","title":"Repair transducer"},{"location":"pipeline/transducers/repair/#repair-transducer","text":"","title":"Repair transducer"},{"location":"pipeline/transducers/repair/#how-it-works","text":"Repair Rules: A repair rule uses data context data to fill in gaps in source data. Repair rules are based on functional dependencies, of the form ( determinant \u2192 dependent ). A functional dependency indicates that a value for the determinant uniquely identifies a value for the dependent .","title":"How it works"},{"location":"pipeline/transducers/repair/#an-example","text":"In Data Preparer, functional dependencies are inferred from the data context as shown in the image. In practice, whenever the attributes in a functional dependency found in the data context can be aligned with attributes in a data set, missing values for the dependent attribute in the data set can be replaced with the corresponding value from the data context. For example, the dependency ( MasterData.title \u2192 MasterData.rating ) can be used to populate missing values for rating in Riverside books, and we can see that 16 values for Riverside.rating have been updated using this dependency.","title":"An example"},{"location":"pipeline/transducers/repair/#configurations-in-the-control-panel","text":"","title":"Configurations in the control panel"},{"location":"pipeline/transducers/repair/#format-match-parameters","text":"Apply rules : Leave the box: - unchecked in order to verify whether to apply or discard suggested repairs. - checked to repair sources directly, based on the discovered rules, in which case you can still view applied repairs.git add","title":"Format match parameters:"},{"location":"quick-start/","text":"","title":"Index"},{"location":"quick-start/advanced/","text":"","title":"Advanced"},{"location":"quick-start/basic/","text":"","title":"Basic"},{"location":"quick-start/install/","text":"Install Datapreparer In this section, we will be going through the steps to install the datapreparer on your computer. Clone Datapreparer from Github Datapreparer can be cloned from The Data Value Factory's Github . Using Docker Docker support it unavaliable currently but we will be added in due course.","title":"Install Datapreparer"},{"location":"quick-start/install/#install-datapreparer","text":"In this section, we will be going through the steps to install the datapreparer on your computer.","title":"Install Datapreparer"},{"location":"quick-start/install/#clone-datapreparer-from-github","text":"Datapreparer can be cloned from The Data Value Factory's Github .","title":"Clone Datapreparer from Github"},{"location":"quick-start/install/#using-docker","text":"Docker support it unavaliable currently but we will be added in due course.","title":"Using Docker"},{"location":"quick-start/intellij-setup/","text":"IntelliJ setup NOTE: The project now includes a .idea folder which should handle all the set up for you. This guide is in case that setup does not work as expected. This section will go through the steps to set up the datapreparer tool to run in IntelliJ IDEA. Prerequisites Running the code in IntelliJ requires the following prerequisites: IntelliJ idea installed on your local machine Java 11 Maven 3 The codebase cloned into an IntelliJ project and imported as a maven project with a Java 11 SDK configured. Setting the maven project Go to File > Project structure or \u2318 + ; . After doing this the following menu will be displayed: At this point, you should set the project SDK to Java 11 and the language level. Go to the modules tab, press the blue + button and choose the new module and the maven option. After completing this step, you should get the following menu, Click the next button. Use the prompt to choose the name and location of the module at this point. Keep the default name. The completed configuration should look like this, A maven project sidebar should be visible. IntelliJ will now look like this, Fixing Java version in maven You now need to set the Java version in the pom.xml file. You can do this by adding the following lines to pom.xml inside the app directory: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.1</version> <configuration> <source>11</source> <target>11</target> </configuration> </plugin> Adding the datapreparer maven configuration We now need to add the maven configuration for the app directory, which contains the application source code. If you look at the maven tab, you will see a small plus button. Click on the small plus button in the maven tab as shown: Click on this add button. The following prompt is to select the location of the pom.xml file for the configuration you would like to add. Navigate to the app directory and select the pom.xml file inside of it. Creating run configurations The final step is the run configuration. We go into the top right-hand corner next to the run button. There is a dropdown to its left; click this dropdown and select edit configurations... . There you should create a new Spring Boot runner. The configuration which is given below, IMPORTANT: the JVM option must be set to -Djava.awt.headless=false -Dapple.awt.UIElement=true -Dtextdb.allow_full_path=true for the application to run correctly. Running the datapreparer app This step is straightforward. Simply select the created run configuration from the dropdown and press the green run button. Congratulations, you now have datapreparer setup with IntelliJ IDEA.","title":"IntelliJ Setup"},{"location":"quick-start/intellij-setup/#intellij-setup","text":"NOTE: The project now includes a .idea folder which should handle all the set up for you. This guide is in case that setup does not work as expected. This section will go through the steps to set up the datapreparer tool to run in IntelliJ IDEA.","title":"IntelliJ setup"},{"location":"quick-start/intellij-setup/#prerequisites","text":"Running the code in IntelliJ requires the following prerequisites: IntelliJ idea installed on your local machine Java 11 Maven 3 The codebase cloned into an IntelliJ project and imported as a maven project with a Java 11 SDK configured.","title":"Prerequisites"},{"location":"quick-start/intellij-setup/#setting-the-maven-project","text":"Go to File > Project structure or \u2318 + ; . After doing this the following menu will be displayed: At this point, you should set the project SDK to Java 11 and the language level. Go to the modules tab, press the blue + button and choose the new module and the maven option. After completing this step, you should get the following menu, Click the next button. Use the prompt to choose the name and location of the module at this point. Keep the default name. The completed configuration should look like this, A maven project sidebar should be visible. IntelliJ will now look like this,","title":"Setting the maven project"},{"location":"quick-start/intellij-setup/#fixing-java-version-in-maven","text":"You now need to set the Java version in the pom.xml file. You can do this by adding the following lines to pom.xml inside the app directory: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.1</version> <configuration> <source>11</source> <target>11</target> </configuration> </plugin>","title":"Fixing Java version in maven"},{"location":"quick-start/intellij-setup/#adding-the-datapreparer-maven-configuration","text":"We now need to add the maven configuration for the app directory, which contains the application source code. If you look at the maven tab, you will see a small plus button. Click on the small plus button in the maven tab as shown: Click on this add button. The following prompt is to select the location of the pom.xml file for the configuration you would like to add. Navigate to the app directory and select the pom.xml file inside of it.","title":"Adding the datapreparer maven configuration"},{"location":"quick-start/intellij-setup/#creating-run-configurations","text":"The final step is the run configuration. We go into the top right-hand corner next to the run button. There is a dropdown to its left; click this dropdown and select edit configurations... . There you should create a new Spring Boot runner. The configuration which is given below, IMPORTANT: the JVM option must be set to -Djava.awt.headless=false -Dapple.awt.UIElement=true -Dtextdb.allow_full_path=true for the application to run correctly.","title":"Creating run configurations"},{"location":"quick-start/intellij-setup/#running-the-datapreparer-app","text":"This step is straightforward. Simply select the created run configuration from the dropdown and press the green run button. Congratulations, you now have datapreparer setup with IntelliJ IDEA.","title":"Running the datapreparer app"},{"location":"quick-start/setup/","text":"","title":"Setup"},{"location":"quick-start/starter-project/","text":"Example Starter Project In this section, we will use the datapreparer web interface to introduce a running example that will illustrate the features of Data Preparer. The sample is small enough to be easily understood and presents several data integration and cleaning challenges that need to be resolved during data preparation. Prerequisites To run the example project, you need to have the datapreparer setup and the ManualBooks0.scenario file. This file should have come with the download, or you can download it manually from here. todo Introduction The scenario involves an online bookshop that is considering introducing promotions on specific authors and is interested in comparing the prices of its competitors for these authors. With this in mind, it has scraped data on the publications by the authors from several competitor sites to act as sources for the data preparation process. Two of these (with some example data) are for Rainforest Books (Table 1) and Riverside Books (Table 2). The tables contain certain features that need to be considered during data wrangling that is common in practice (e.g., varying numbers of columns, inconsistent names for columns having the same data, missing values). Launching the web interface Let's launch the web interface (full details in the previous section) to get started. If you have launched it correctly, you should see this page. Import the scenario Next, let's import the ManualBooks0.scenario file to get some data. The import scenario button is right next to the wrangle button. Once the scenario is imported, you will see that the scenario info section is now populated with information from the imported scenario. Viewing the data We can take a more detailed look at the data by navigating to the data source tab. This tab displays all the data sources for the scenario. You can also view the data rows directly by clicking on any data sources. For example, here are the data rows for the GoodReads data source. Visualisation of the data For any data table in our scenario, we can visualise its attribute values by clicking the respective chart icon, thus facilitating the discovery of outliers, anomalies, patterns, trends in the data, etc. Furthermore, we can visualise tables in sources or data context, candidate mappings, and the end product. How attributes are plotted Attributes with numeric values are plotted as continuous lines, along with a statistics summary (maximum, minimum, mean, standard deviation, median, and sum). Attributes with many values, a distributed sample of a maximum of 1000 values will be plotted, while the statistics summary will still be calculated on the complete data, ignoring empty values. Attributes with string values will be plotted as bar charts, in which the 9 first bar heights correspond to the 9 most frequent terms, while the last item in the horizontal axis refers to the number of the values that are not plotted. Here is the visualisation of the attributes in the GoodReads table. Setting the target The target section is where you would set your expected output format for the data; this is how the user makes explicit what is required of the wrangling process. Then, the datapreparer will transform the input data sources to fit this format. The target for data preparation is also a table definition, just add the fields you want, and the datapreparer will try to match the data sources to them. Wrangle the data With this information (i.e. the sources and the target), it is already possible to wrangle the data with a single click; the wrangle option is always available on the left side of the submenu. This will give rise to a result containing several rows specified in the control panel. Note: Configurations are set in the control panel section, but we will cover that in a different guide. You can view the output rows here in the end product tab. Note: The end product can still contain blank cells, so further data cleaning must be done. You can let datapreparer handle this by setting further configurations, but the datapreparer has managed to link fields from different tables into just one table. Viewing the results at each stage of the workflow In the control panel tab, you can view the workflow of the wrangling process. The workflow contains multiple transducers which transform data from one form to another at multiple stages. We will be covering how these transducers work in detail in another section. You can view the output from each transducer by clicking on any of them. End of example Congratulations! You have finished going through the basics with datapreparer. Let's review what we just went through What just happened? With very little evidence, Data Preparer has had a go at wrangling a collection of sources into a target \u2013 here only a few sources, but there could have been many more. Why is this good? This is good because in other data preparation systems significant manual effort is required to carry out even preliminary wrangling over a collection of sources. In other data preparation systems, manual effort tends to be required for each data set and each wrangling step, so data context helps to provide cost-effective data preparation. What more is needed? So far, no indication has been provided as to what result features are most important. With automated data preparation, alternative results can be produced rapidly, so an opportunity exists to tailor the wrangling process to the task in hand. (Look at the setting priorities section ) In Data Preparer, an initial result can be obtained much more quickly. Of course, this initial result can be improved upon, but data preparation has been essentially hands-off.","title":"Starter Example"},{"location":"quick-start/starter-project/#example-starter-project","text":"In this section, we will use the datapreparer web interface to introduce a running example that will illustrate the features of Data Preparer. The sample is small enough to be easily understood and presents several data integration and cleaning challenges that need to be resolved during data preparation.","title":"Example Starter Project"},{"location":"quick-start/starter-project/#prerequisites","text":"To run the example project, you need to have the datapreparer setup and the ManualBooks0.scenario file. This file should have come with the download, or you can download it manually from here. todo","title":"Prerequisites"},{"location":"quick-start/starter-project/#introduction","text":"The scenario involves an online bookshop that is considering introducing promotions on specific authors and is interested in comparing the prices of its competitors for these authors. With this in mind, it has scraped data on the publications by the authors from several competitor sites to act as sources for the data preparation process.","title":"Introduction"},{"location":"quick-start/starter-project/#two-of-these-with-some-example-data-are-for","text":"Rainforest Books (Table 1) and Riverside Books (Table 2). The tables contain certain features that need to be considered during data wrangling that is common in practice (e.g., varying numbers of columns, inconsistent names for columns having the same data, missing values).","title":"Two of these (with some example data) are for"},{"location":"quick-start/starter-project/#launching-the-web-interface","text":"Let's launch the web interface (full details in the previous section) to get started. If you have launched it correctly, you should see this page.","title":"Launching the web interface"},{"location":"quick-start/starter-project/#import-the-scenario","text":"Next, let's import the ManualBooks0.scenario file to get some data. The import scenario button is right next to the wrangle button. Once the scenario is imported, you will see that the scenario info section is now populated with information from the imported scenario.","title":"Import the scenario"},{"location":"quick-start/starter-project/#viewing-the-data","text":"We can take a more detailed look at the data by navigating to the data source tab. This tab displays all the data sources for the scenario. You can also view the data rows directly by clicking on any data sources. For example, here are the data rows for the GoodReads data source.","title":"Viewing the data"},{"location":"quick-start/starter-project/#visualisation-of-the-data","text":"For any data table in our scenario, we can visualise its attribute values by clicking the respective chart icon, thus facilitating the discovery of outliers, anomalies, patterns, trends in the data, etc. Furthermore, we can visualise tables in sources or data context, candidate mappings, and the end product.","title":"Visualisation of the data"},{"location":"quick-start/starter-project/#how-attributes-are-plotted","text":"Attributes with numeric values are plotted as continuous lines, along with a statistics summary (maximum, minimum, mean, standard deviation, median, and sum). Attributes with many values, a distributed sample of a maximum of 1000 values will be plotted, while the statistics summary will still be calculated on the complete data, ignoring empty values. Attributes with string values will be plotted as bar charts, in which the 9 first bar heights correspond to the 9 most frequent terms, while the last item in the horizontal axis refers to the number of the values that are not plotted. Here is the visualisation of the attributes in the GoodReads table.","title":"How attributes are plotted"},{"location":"quick-start/starter-project/#setting-the-target","text":"The target section is where you would set your expected output format for the data; this is how the user makes explicit what is required of the wrangling process. Then, the datapreparer will transform the input data sources to fit this format. The target for data preparation is also a table definition, just add the fields you want, and the datapreparer will try to match the data sources to them.","title":"Setting the target"},{"location":"quick-start/starter-project/#wrangle-the-data","text":"With this information (i.e. the sources and the target), it is already possible to wrangle the data with a single click; the wrangle option is always available on the left side of the submenu. This will give rise to a result containing several rows specified in the control panel. Note: Configurations are set in the control panel section, but we will cover that in a different guide. You can view the output rows here in the end product tab. Note: The end product can still contain blank cells, so further data cleaning must be done. You can let datapreparer handle this by setting further configurations, but the datapreparer has managed to link fields from different tables into just one table.","title":"Wrangle the data"},{"location":"quick-start/starter-project/#viewing-the-results-at-each-stage-of-the-workflow","text":"In the control panel tab, you can view the workflow of the wrangling process. The workflow contains multiple transducers which transform data from one form to another at multiple stages. We will be covering how these transducers work in detail in another section. You can view the output from each transducer by clicking on any of them.","title":"Viewing the results at each stage of the workflow"},{"location":"quick-start/starter-project/#end-of-example","text":"Congratulations! You have finished going through the basics with datapreparer.","title":"End of example"},{"location":"quick-start/starter-project/#lets-review-what-we-just-went-through","text":"What just happened? With very little evidence, Data Preparer has had a go at wrangling a collection of sources into a target \u2013 here only a few sources, but there could have been many more. Why is this good? This is good because in other data preparation systems significant manual effort is required to carry out even preliminary wrangling over a collection of sources. In other data preparation systems, manual effort tends to be required for each data set and each wrangling step, so data context helps to provide cost-effective data preparation. What more is needed? So far, no indication has been provided as to what result features are most important. With automated data preparation, alternative results can be produced rapidly, so an opportunity exists to tailor the wrangling process to the task in hand. (Look at the setting priorities section ) In Data Preparer, an initial result can be obtained much more quickly. Of course, this initial result can be improved upon, but data preparation has been essentially hands-off.","title":"Let's review what we just went through"},{"location":"quick-start/vscode-setup/","text":"Setup using Visual Studio Code NOTE: These steps should work for Windows, WSL, Linux and MacOS In this section, we will be going through the steps to setup the datapreparer tool to run in visual studio code. Prerequisites Install the Java and Extension Pack for Java extensions. Creating a launch configuration Head over to the Run and Debug extension on the left of your application. Click on create a launch.json file . Select Java . A launch.json file should now be created for you. NOTE: It should take a minute or two to generate this file. Once it is created, go back to the Run and Debug extension. Click on the drop down menu and click add configuration . Add this configuration into your launch.js. Make sure that vmArgs is there. { \"type\": \"java\", \"name\": \"Launch Data Preparer\", \"request\": \"launch\", \"mainClass\": \"com.tdvf.app.Main\", \"projectName\": \"datapreparer\", \"vmArgs\": \"-Xms768m -Djava.awt.headless=false -Dapple.awt.UIElement=true -Dtextdb.allow_full_path=true\" } Run the Datapreparer app Go back to the Run and Debug extension and select Launch Data Preparer, then click on the green start button. Congratulations you now have datapreparer setup with Visual Studio Code.","title":"Vscode Setup"},{"location":"quick-start/vscode-setup/#setup-using-visual-studio-code","text":"NOTE: These steps should work for Windows, WSL, Linux and MacOS In this section, we will be going through the steps to setup the datapreparer tool to run in visual studio code.","title":"Setup using Visual Studio Code"},{"location":"quick-start/vscode-setup/#prerequisites","text":"Install the Java and Extension Pack for Java extensions.","title":"Prerequisites"},{"location":"quick-start/vscode-setup/#creating-a-launch-configuration","text":"Head over to the Run and Debug extension on the left of your application. Click on create a launch.json file . Select Java . A launch.json file should now be created for you. NOTE: It should take a minute or two to generate this file. Once it is created, go back to the Run and Debug extension. Click on the drop down menu and click add configuration . Add this configuration into your launch.js. Make sure that vmArgs is there. { \"type\": \"java\", \"name\": \"Launch Data Preparer\", \"request\": \"launch\", \"mainClass\": \"com.tdvf.app.Main\", \"projectName\": \"datapreparer\", \"vmArgs\": \"-Xms768m -Djava.awt.headless=false -Dapple.awt.UIElement=true -Dtextdb.allow_full_path=true\" }","title":"Creating a launch configuration"},{"location":"quick-start/vscode-setup/#run-the-datapreparer-app","text":"Go back to the Run and Debug extension and select Launch Data Preparer, then click on the green start button. Congratulations you now have datapreparer setup with Visual Studio Code.","title":"Run the Datapreparer app"}]}